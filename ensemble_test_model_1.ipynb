{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdefa926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_torch_dataset import SwipeDataset\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset\")\n",
    "\n",
    "data = SwipeDataset(data_dir=dataset_path,\n",
    "                    batch=False)\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1], generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baea1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch: List of tuples (input, word, word_tensor)\n",
    "                  - input: (T, 6)\n",
    "                  - word: a string of characters\n",
    "                  - word_tensor: encoded word as indicies with 0 as the blank\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort batch by sequence length (descending order)\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "\n",
    "    inputs, words, targets = zip(*batch)\n",
    "    input_lengths = torch.LongTensor([x.shape[0] for x in inputs])  # store the lengths of inputs\n",
    "    input = pack_sequence(inputs)   # pack the inputs\n",
    "\n",
    "    target_lengths = torch.LongTensor([len(x) for x in words])\n",
    "    targets = torch.cat(targets)    # concatenate all the targets\n",
    "\n",
    "    return input, targets, input_lengths, target_lengths, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb4ef7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "dataloaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430db5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_sequence\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class CTCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, output_size=27, bidirectional=True, dropout = 0.5, lstm_dropout = 0):\n",
    "        super(CTCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=False,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=lstm_dropout)\n",
    "        self.layer_norm = nn.LayerNorm(2 * hidden_size if bidirectional else hidden_size)\n",
    "        # self.batch_norm = nn.BatchNorm1d(2 * hidden_size if bidirectional else hidden_size)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size if bidirectional else hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        : param x: (batch_size, seq_len, input_size)\n",
    "        : return logits: (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        lstm_outputs, _ = self.lstm(x)  # lstm_out shape: (seq_len, batch_size, hidden_size*2 if bidirectional)\n",
    "\n",
    "        lstm_out = pad_packed_sequence(lstm_outputs, batch_first=False)[0]\n",
    "\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.drop(lstm_out)\n",
    "\n",
    "        # lstm_out = self.batch_norm(lstm_out.permute(0, 2, 1))\n",
    "        # lstm_out = self.drop(lstm_out.permute(0, 2, 1))\n",
    "\n",
    "        logits = self.fc(lstm_out)  # shape: (batch_size, seq_len, output_size)\n",
    "        return F.log_softmax(logits, dim=-1)  # Log-softmax for CTC loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ebd1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_file = \"lstm_n.pt\"\n",
    "save_path = os.path.join(os.getcwd(), \"models\", model_file)\n",
    "\n",
    "t_model = torch.load(save_path, weights_only=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3726ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, datapoints):\n",
    "    criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    model_outputs = []\n",
    "    ground_truth = []\n",
    "    # set the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, targets, input_lengths, target_lengths, words in test_data:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        model_outputs.append(outputs.cpu())\n",
    "        ground_truth.append(words)\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        running_loss += loss * len(target_lengths)  # multiply by batch size\n",
    "\n",
    "    avg_loss = running_loss / datapoints    # average over the entire test set\n",
    "\n",
    "    print(f\"Average test loss: {avg_loss}\")\n",
    "\n",
    "    return model_outputs, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54b5d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.1748939752578735\n"
     ]
    }
   ],
   "source": [
    "logits, truth = test_model(t_model, test_loader, len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1079efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'_': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8,\n",
    "              'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16,\n",
    "              'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24,\n",
    "              'y': 25, 'z': 26}\n",
    "reversed_vocab = {k: u for u, k in vocabulary.items()}\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "\n",
    "def beam_search(logits, ground_truth, beam_width=3):\n",
    "    \"\"\"\n",
    "    Decodes a tensor of logits using beam search, adapted for the provided input structure.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): A tensor of logits with shape (sequence_length, batch_size, vocab_size).\n",
    "        ground_truth (list): A list of ground truth words (not directly used in beam search but included for consistency).\n",
    "        reversed_vocab (dict): A dictionary mapping vocabulary indices to characters.\n",
    "        beam_width (int): The width of the beam.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of decoded strings, one for each sequence in the batch.\n",
    "    \"\"\"\n",
    "    decoded_words = []\n",
    "    for batch, _ in zip(logits, ground_truth):  # ground_truth is not used in the beam search.\n",
    "        batched_results = []\n",
    "\n",
    "        for b in range(batch.shape[1]):\n",
    "            initial_beam = [([(0.0, \"\")], 0.0)]\n",
    "            final_beams = []\n",
    "\n",
    "            for t in range(batch.shape[0]):\n",
    "                new_beam = []\n",
    "                for seq, total_log_prob in initial_beam:\n",
    "                    timestep_logits = batch[t, b, :]\n",
    "\n",
    "                    topk_probs, topk_indices = torch.topk(timestep_logits, beam_width)\n",
    "\n",
    "                    for i in range(beam_width):\n",
    "                        char_index = topk_indices[i].item()\n",
    "                        char = reversed_vocab.get(char_index, '_')\n",
    "                        new_seq = seq + [(topk_probs[i].item(), char)]\n",
    "                        new_total_log_prob = total_log_prob + topk_probs[i].item()\n",
    "                        new_beam.append((new_seq, new_total_log_prob))\n",
    "\n",
    "                initial_beam = heapq.nlargest(beam_width, new_beam, key=lambda x: x[1])\n",
    "\n",
    "            best_sequence, _ = max(initial_beam, key=lambda x: x[1])\n",
    "            decoded_word = \"\".join([char for log_prob, char in best_sequence[1:] if char != \"_\"])\n",
    "            batched_results.append(decoded_word)\n",
    "\n",
    "        decoded_words.append(batched_results)\n",
    "\n",
    "    return decoded_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e8bf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_words = beam_search(logits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d395f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, truth):\n",
    "    exact_match = 0\n",
    "    length_match = 0\n",
    "    same_first = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            pred = predictions[i][j]\n",
    "            ground = truth[i][j]\n",
    "            total += 1\n",
    "\n",
    "            if pred == ground:\n",
    "                exact_match += 1\n",
    "            \n",
    "            if len(pred) == len(ground):\n",
    "                length_match += 1\n",
    "            \n",
    "            if pred[0] == ground[0]:\n",
    "                same_first += 1\n",
    "    \n",
    "\n",
    "    EM = exact_match / total\n",
    "    LM = length_match / total\n",
    "    FM = same_first / total\n",
    "\n",
    "    print(f\"Exact Match: {EM}\")\n",
    "    print(f\"Correct Length: {LM}\")\n",
    "    print(f\"Correct First Letter: {FM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62de4beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.24599434495758718\n",
      "Correct Length: 0.4665409990574929\n",
      "Correct First Letter: 0.7224316682375118\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_words, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67a424d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "path_to_model = \"ai-forever/T5-large-spell\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(path_to_model).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c2826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everything']\n"
     ]
    }
   ],
   "source": [
    "prefix = \"spelling: \"\n",
    "sentence = prefix + beam_words[0][0]\n",
    "\n",
    "encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "encodings = encodings.to(device)\n",
    "generated_tokens = model.generate(**encodings)\n",
    "answer = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prefix = \"grammar: \"\n",
    "\n",
    "def autocorrect(predictions):\n",
    "    autocorrected_words = []\n",
    "\n",
    "    for batch in tqdm(predictions):\n",
    "        sentences = [prefix + word for word in batch]\n",
    "        encodings = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "        encodings = encodings.to(device)\n",
    "        generated_tokens = model.generate(**encodings)\n",
    "        answers = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        autocorrected_words.append(answers)\n",
    "\n",
    "    return autocorrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1c6c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:51<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "final_predictions = autocorrect(beam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc6561da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autocorrect(predictions, truth):\n",
    "    exact_match = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            pred = predictions[i][j].lower()\n",
    "            ground = truth[i][j]\n",
    "            total += 1\n",
    "\n",
    "            if pred == ground:\n",
    "                exact_match += 1\n",
    "    \n",
    "\n",
    "    EM = exact_match / total\n",
    "\n",
    "    print(f\"Exact Match: {EM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa7dee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.17436380772855797\n"
     ]
    }
   ],
   "source": [
    "evaluate_autocorrect(final_predictions, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
