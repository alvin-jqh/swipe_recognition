{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61af38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_torch_dataset import SwipeDataset\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset\")\n",
    "\n",
    "data = SwipeDataset(data_dir=dataset_path,\n",
    "                    batch=False)\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1], generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f5c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch: List of tuples (input, word, word_tensor)\n",
    "                  - input: (T, 6)\n",
    "                  - word: a string of characters\n",
    "                  - word_tensor: encoded word as indicies with 0 as the blank\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort batch by sequence length (descending order)\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    inputs, words, targets = zip(*batch)\n",
    "    input_lengths = torch.LongTensor([x.shape[0] for x in inputs])  # store the lengths of inputs\n",
    "\n",
    "    target_lengths = torch.LongTensor([len(x) for x in words])\n",
    "    targets = torch.cat(targets)    # concatenate all the targets\n",
    "\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True)  # pad inputs to max length with zeros   (B, T, 6)\n",
    "\n",
    "    return padded_inputs, targets, input_lengths, target_lengths, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad42182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "dataloaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df96367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class SwipeToTextCTC(nn.Module):\n",
    "    def __init__(self, input_size=6, conv_channels=32, hidden_size=128,\n",
    "                 num_layers=2, output_size=27, bidirectional=True,\n",
    "                 dropout=0.1):\n",
    "        super(SwipeToTextCTC, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=conv_channels, kernel_size=5, padding=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(conv_channels)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        lstm_out_dim = 2 * hidden_size if bidirectional else hidden_size\n",
    "        self.layer_norm2 = nn.LayerNorm(lstm_out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(lstm_out_dim, output_size)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        \"\"\"\n",
    "        x: (B, T, 6) = (batch, sequence length, feature dim)\n",
    "        output: (T, B, output_size)\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1)         # (B, 6, T)\n",
    "        x = self.conv(x)               # (B, conv_channels, T)\n",
    "        x = x.permute(2, 0, 1)         # (T, B, conv_channels)\n",
    "\n",
    "        x = self.layer_norm1(x)\n",
    "        lstm_in = pack_padded_sequence(x, input_lengths.cpu(), batch_first=False)\n",
    "        lstm_outputs, _ = self.lstm(lstm_in)   \n",
    "        lstm_out = pad_packed_sequence(lstm_outputs, batch_first=False)[0]  \n",
    "\n",
    "        lstm_out = self.layer_norm2(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        logits = self.fc(lstm_out)     # (T, B, output_size)\n",
    "        \n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4480fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class SwipeToTextCTC(nn.Module):\n",
    "    def __init__(self, input_size=6, conv_channels=32, kernel_size = 5,\n",
    "                 hidden_size=128, num_layers=2, output_size=27, bidirectional=True,\n",
    "                 dropout=0.1):\n",
    "        super(SwipeToTextCTC, self).__init__()\n",
    "\n",
    "        padding = kernel_size // 2 \n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=conv_channels, kernel_size=kernel_size, padding=padding, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.norm_drop1 = nn.Sequential(\n",
    "            nn.LayerNorm(conv_channels),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        lstm_out_dim = 2 * hidden_size if bidirectional else hidden_size\n",
    "\n",
    "        self.norm_drop2 = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_out_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(lstm_out_dim, output_size)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        \"\"\"\n",
    "        x: (B, T, 6) = (batch, sequence length, feature dim)\n",
    "        output: (T, B, output_size)\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1)         # (B, 6, T)\n",
    "        x = self.conv(x)               # (B, conv_channels, T)\n",
    "        x = x.permute(2, 0, 1)         # (T, B, conv_channels)\n",
    "\n",
    "        x = self.norm_drop1(x)\n",
    "        lstm_in = pack_padded_sequence(x, input_lengths.cpu(), batch_first=False)\n",
    "        lstm_outputs, _ = self.lstm(lstm_in)   \n",
    "        lstm_out = pad_packed_sequence(lstm_outputs, batch_first=False)[0]  \n",
    "\n",
    "        lstm_out = self.norm_drop2(lstm_out)\n",
    "        logits = self.fc(lstm_out)     # (T, B, output_size)\n",
    "        \n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4ff1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_file = \"conv_32_k11_128_4.pt\"\n",
    "save_path = os.path.join(os.getcwd(), \"models\", model_file)\n",
    "\n",
    "t_model = torch.load(save_path, weights_only=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b60db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwipeToTextCTC(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv1d(6, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (norm_drop1): Sequential(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(32, 128, num_layers=4, dropout=0.3, bidirectional=True)\n",
       "  (norm_drop2): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d84df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, datapoints):\n",
    "    criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    model_outputs = []\n",
    "    ground_truth = []\n",
    "    # set the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, targets, input_lengths, target_lengths, words in test_data:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        outputs = model(inputs, input_lengths)\n",
    "        model_outputs.append(outputs.cpu())\n",
    "        ground_truth.append(words)\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        running_loss += loss * len(target_lengths)  # multiply by batch size\n",
    "\n",
    "    avg_loss = running_loss / datapoints    # average over the entire test set\n",
    "\n",
    "    print(f\"Average test loss: {avg_loss}\")\n",
    "\n",
    "    return model_outputs, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac03b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 1.1674144268035889\n"
     ]
    }
   ],
   "source": [
    "logits, truth = test_model(t_model, test_loader, len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d24b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'_': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8,\n",
    "              'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16,\n",
    "              'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24,\n",
    "              'y': 25, 'z': 26}\n",
    "reversed_vocab = {k: u for u, k in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a77ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "\n",
    "def beam_search(logits, ground_truth, beam_width=3):\n",
    "    \"\"\"\n",
    "    Decodes a tensor of logits using beam search, adapted for the provided input structure.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): A tensor of logits with shape (sequence_length, batch_size, vocab_size).\n",
    "        ground_truth (list): A list of ground truth words (not directly used in beam search but included for consistency).\n",
    "        reversed_vocab (dict): A dictionary mapping vocabulary indices to characters.\n",
    "        beam_width (int): The width of the beam.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of decoded strings, one for each sequence in the batch.\n",
    "    \"\"\"\n",
    "    decoded_words = []\n",
    "    for batch, _ in zip(logits, ground_truth):  # ground_truth is not used in the beam search.\n",
    "        batched_results = []\n",
    "\n",
    "        for b in range(batch.shape[1]):\n",
    "            initial_beam = [([(0.0, \"\")], 0.0)]\n",
    "            final_beams = []\n",
    "\n",
    "            for t in range(batch.shape[0]):\n",
    "                new_beam = []\n",
    "                for seq, total_log_prob in initial_beam:\n",
    "                    timestep_logits = batch[t, b, :]\n",
    "\n",
    "                    topk_probs, topk_indices = torch.topk(timestep_logits, beam_width)\n",
    "\n",
    "                    for i in range(beam_width):\n",
    "                        char_index = topk_indices[i].item()\n",
    "                        char = reversed_vocab.get(char_index, '_')\n",
    "                        new_seq = seq + [(topk_probs[i].item(), char)]\n",
    "                        new_total_log_prob = total_log_prob + topk_probs[i].item()\n",
    "                        new_beam.append((new_seq, new_total_log_prob))\n",
    "\n",
    "                initial_beam = heapq.nlargest(beam_width, new_beam, key=lambda x: x[1])\n",
    "\n",
    "            best_sequence, _ = max(initial_beam, key=lambda x: x[1])\n",
    "            decoded_word = \"\".join([char for log_prob, char in best_sequence[1:] if char != \"_\"])\n",
    "            batched_results.append(decoded_word)\n",
    "\n",
    "        decoded_words.append(batched_results)\n",
    "\n",
    "    return decoded_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39d9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_words = beam_search(logits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebb104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, truth):\n",
    "    exact_match = 0\n",
    "    length_match = 0\n",
    "    same_first = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            pred = predictions[i][j]\n",
    "            ground = truth[i][j]\n",
    "            total += 1\n",
    "\n",
    "            if pred == ground:\n",
    "                exact_match += 1\n",
    "            \n",
    "            if len(pred) == len(ground):\n",
    "                length_match += 1\n",
    "            \n",
    "            if pred[0] == ground[0]:\n",
    "                same_first += 1\n",
    "    \n",
    "\n",
    "    EM = exact_match / total\n",
    "    LM = length_match / total\n",
    "    FM = same_first / total\n",
    "\n",
    "    print(f\"Exact Match: {EM}\")\n",
    "    print(f\"Correct Length: {LM}\")\n",
    "    print(f\"Correct First Letter: {FM}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a1ddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.2648444863336475\n",
      "Correct Length: 0.4934024505183789\n",
      "Correct First Letter: 0.735626767200754\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_words, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3161553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "path_to_model = \"ai-forever/T5-large-spell\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(path_to_model).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8dc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prefix = \"grammar: \"\n",
    "\n",
    "def autocorrect(predictions):\n",
    "    autocorrected_words = []\n",
    "\n",
    "    for batch in tqdm(predictions):\n",
    "        sentences = [prefix + word for word in batch]\n",
    "        encodings = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "        encodings = encodings.to(device)\n",
    "        generated_tokens = model.generate(**encodings)\n",
    "        answers = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        autocorrected_words.append(answers)\n",
    "\n",
    "    return autocorrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dadac812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [02:59<00:00, 10.56s/it]\n"
     ]
    }
   ],
   "source": [
    "final_predictions = autocorrect(beam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa8d8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autocorrect(predictions, truth):\n",
    "    exact_match = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            pred = predictions[i][j].lower()\n",
    "            ground = truth[i][j]\n",
    "            total += 1\n",
    "\n",
    "            if pred == ground:\n",
    "                exact_match += 1\n",
    "    \n",
    "\n",
    "    EM = exact_match / total\n",
    "\n",
    "    print(f\"Exact Match: {EM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f68063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.10980207351555137\n"
     ]
    }
   ],
   "source": [
    "evaluate_autocorrect(final_predictions, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
