{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_torch_dataset import SwipeDataset\n",
    "import os\n",
    "\n",
    "data = SwipeDataset(os.path.join(os.getcwd(), \"dataset\"), batch=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{data[0][0]}\")\n",
    "print(f\"{data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{data[0][2]}\")\n",
    "print(f\"{data[0][2].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][2][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.Tensor([0]), data[0][2].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,)\n",
    "        self.fc = nn.Linear(2*hidden_size,hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch_size, input_size)      seq len would be the number of touchpoints\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # if self.bidirectional:\n",
    "        #     outputs = self.fc(outputs)\n",
    "        # outputs: (seq_len, batch_size, hidden_size)\n",
    "        # hidden, cell: (num_layers, batch_size, hidden_size)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.3466e-02, -5.8422e-02,  5.7168e-02,  ..., -1.1227e-02,\n",
      "           9.7457e-02,  1.1600e-01],\n",
      "         [-5.9514e-02, -2.0010e-02,  7.6389e-02,  ...,  4.5899e-02,\n",
      "           7.3622e-02,  2.3051e-01],\n",
      "         [-4.3995e-02,  3.5770e-02,  6.4889e-02,  ...,  6.9043e-02,\n",
      "           6.6774e-02,  2.8291e-01],\n",
      "         ...,\n",
      "         [ 4.2326e-05,  2.5941e-01,  1.0277e-01,  ...,  4.9886e-03,\n",
      "          -2.3909e-02,  2.1044e-01],\n",
      "         [ 1.7118e-03,  2.6129e-01,  1.0233e-01,  ...,  5.6349e-03,\n",
      "          -3.3850e-02,  1.6091e-01],\n",
      "         [ 4.2687e-03,  2.5741e-01,  9.1282e-02,  ...,  3.3023e-03,\n",
      "          -3.7539e-02,  8.8736e-02]]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward0>)\n",
      "tensor([[[ 1.6683e-01,  7.6160e-01, -7.6159e-01,  5.9580e-19,  2.3287e-12,\n",
      "          -1.2129e-01, -3.2210e-02,  8.6411e-03, -3.1395e-03,  5.9091e-04,\n",
      "          -1.0000e+00, -2.8133e-09,  1.8214e-21, -2.0838e-21,  2.6293e-11,\n",
      "          -3.4006e-25,  7.4989e-02,  1.0000e+00, -7.4796e-01, -3.0850e-03,\n",
      "           7.6159e-01, -1.0000e+00,  8.2266e-09,  7.4649e-06, -5.2383e-05,\n",
      "          -1.0000e+00, -8.8971e-02,  1.1689e-03,  3.0606e-14,  1.6504e-20,\n",
      "          -1.7055e-24, -3.4839e-10,  1.0000e+00, -2.8472e-22, -1.0545e-04,\n",
      "          -6.3479e-10, -1.2867e-24,  1.0000e+00,  2.1000e-04, -8.9150e-06,\n",
      "           1.0000e+00, -4.1718e-03,  1.0000e+00, -2.3849e-01,  6.0179e-13,\n",
      "           4.6944e-25, -9.3294e-04,  2.2533e-02, -3.4663e-02, -5.3621e-10,\n",
      "           7.9588e-39,  7.1136e-01, -6.0251e-01, -7.6159e-01,  7.6124e-01,\n",
      "          -7.6159e-01, -4.0583e-12,  1.5689e-15, -1.0000e+00,  7.9862e-21,\n",
      "          -1.6524e-19,  2.4683e-08,  1.7659e-01,  7.2611e-01]],\n",
      "\n",
      "        [[-9.7310e-02, -3.2289e-02, -4.4284e-01, -4.3308e-01, -3.3703e-03,\n",
      "          -4.4110e-01, -2.1581e-02, -1.4059e-01, -4.2989e-01,  5.7266e-01,\n",
      "           6.1687e-02,  1.1976e-02,  1.7446e-01,  4.2811e-01, -5.0919e-02,\n",
      "           1.7590e-01,  4.9706e-01, -2.9750e-02,  1.6136e-01, -1.7024e-01,\n",
      "           5.4911e-01,  1.7253e-01, -2.3550e-02, -3.5003e-01,  1.4058e-01,\n",
      "           2.2735e-01,  1.4855e-02,  1.4414e-01, -5.5265e-01, -2.3174e-01,\n",
      "          -4.5839e-01,  4.6148e-03, -3.1964e-02,  2.6266e-01,  4.5199e-01,\n",
      "           1.4055e-01, -9.3612e-02,  5.2797e-01, -2.4978e-01, -1.8854e-01,\n",
      "          -1.2089e-01,  1.2499e-01,  1.5640e-01, -2.4350e-02, -4.6062e-01,\n",
      "          -4.2960e-02, -4.0168e-01, -1.1027e-01,  3.7221e-02, -3.8528e-02,\n",
      "           5.8801e-01,  5.0245e-02,  5.7629e-01, -2.1135e-01, -1.3003e-01,\n",
      "          -3.0822e-01,  1.6317e-04, -5.2822e-01,  2.4234e-01, -4.2993e-02,\n",
      "           1.3402e-01,  2.1490e-01, -9.5360e-02,  6.8496e-02]],\n",
      "\n",
      "        [[ 4.2687e-03,  2.5741e-01,  9.1282e-02, -1.7144e-01,  4.9847e-02,\n",
      "           7.2642e-03,  2.1211e-02, -3.2400e-02, -3.7794e-01, -2.3073e-01,\n",
      "           1.3099e-01, -1.8193e-01,  3.5543e-02,  1.7834e-01, -2.5667e-01,\n",
      "          -1.8169e-01, -2.2014e-01, -2.2051e-01,  8.7611e-02,  2.5925e-01,\n",
      "           8.7039e-02,  1.4482e-01, -1.8224e-01,  2.6937e-02, -8.4016e-03,\n",
      "           2.7607e-01, -1.3747e-02,  6.4680e-02, -1.0386e-01, -3.2146e-02,\n",
      "          -4.1363e-02,  1.9249e-01,  2.2117e-02,  1.7347e-01,  3.3570e-01,\n",
      "          -1.3362e-01,  1.5932e-01, -1.0205e-01, -1.7945e-01, -1.0976e-01,\n",
      "          -2.0389e-01, -1.3248e-01,  1.3619e-01, -4.8126e-02, -7.0418e-02,\n",
      "          -1.1286e-01,  8.4111e-02,  2.6695e-01,  8.1508e-02,  2.5027e-01,\n",
      "           4.0042e-01, -2.8845e-02,  1.7818e-01, -3.3729e-01, -1.7146e-01,\n",
      "          -8.6087e-02,  1.2929e-01, -1.9558e-01, -1.1562e-01, -1.8504e-01,\n",
      "          -3.2926e-01,  2.1054e-01,  7.4599e-02,  2.0982e-03]],\n",
      "\n",
      "        [[ 1.4630e-01, -7.8415e-02,  1.4390e-01,  4.5366e-03, -9.4431e-02,\n",
      "           9.1943e-02,  4.8415e-02,  1.4379e-01, -9.3919e-02, -1.5108e-01,\n",
      "          -1.4448e-01, -1.3524e-01,  1.9875e-01, -2.7709e-02,  3.7576e-02,\n",
      "           6.3487e-03,  1.3180e-01,  7.9637e-02, -4.7212e-02, -2.0130e-01,\n",
      "           1.6112e-02, -8.9278e-02, -2.4473e-02,  5.7329e-02,  3.1534e-02,\n",
      "           7.6500e-02, -1.5369e-02, -1.1935e-01,  5.4630e-02, -3.4785e-03,\n",
      "          -1.2437e-01, -1.4879e-01, -6.1962e-02, -7.0706e-02, -1.6864e-01,\n",
      "           1.8878e-01,  1.1438e-01, -3.2035e-02,  1.6291e-01, -1.9939e-03,\n",
      "           1.1598e-02, -1.1919e-01, -2.7294e-02, -1.8505e-02, -8.6053e-02,\n",
      "          -5.6423e-02,  8.3211e-02, -2.9146e-04, -6.2868e-04,  5.0767e-02,\n",
      "           1.9759e-01,  9.0584e-02, -1.4376e-01, -1.5278e-01,  9.8619e-02,\n",
      "          -1.5624e-02,  6.0244e-02, -2.5930e-02, -1.4735e-01, -1.6937e-02,\n",
      "          -2.5476e-01, -1.1227e-02,  9.7457e-02,  1.1600e-01]]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(input_size=6,\n",
    "                  hidden_size=64,\n",
    "                  num_layers=2,\n",
    "                  bidirectional=True,).to(device)\n",
    "\n",
    "en_output, en_hidden, en_cell = encoder(data[0][0].to(device))\n",
    "print(en_output)\n",
    "print(en_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# d_k = 64    \n",
    "# d_v = 64    # should equal hidden size of decoder\n",
    "# N = 16  # batch size\n",
    "# H = 1   # attention heads\n",
    "# w_L = 1 # always 1 because we are doing letter by letter\n",
    "# S_L = 50    # sequence length\n",
    "\n",
    "# # query is the hidden state of encoder or decoder\n",
    "# query = torch.rand(w_L, d_k, dtype=torch.float16, device=\"cuda\")\n",
    "# # key and value are encoder output\n",
    "# key = torch.rand( S_L, d_k, dtype=torch.float16, device=\"cuda\")\n",
    "# value = torch.rand( S_L, d_v, dtype=torch.float16, device=\"cuda\")\n",
    "attention = F.scaled_dot_product_attention(en_hidden.permute(1,0,2),en_output,en_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128       # decoder hidden size is double that of the encoder if encoder is bidirectional\n",
    "output_size = 27\n",
    "\n",
    "test_emb=nn.Embedding(output_size,hidden_size).to(device)\n",
    "test_lstm = nn.LSTM(input_size=2*hidden_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    bidirectional=False,\n",
    "                    batch_first=True).to(device)\n",
    "test_fc = nn.Linear(hidden_size, output_size).to(device)    # the input shape of linear is 2*hidden if lstm is bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "first_input =  torch.tensor([0] * batch_size, dtype=torch.long, device=device).unsqueeze(1)     # (N, 1), 1 because only one letter at a time\n",
    "emb_first_input = test_emb(first_input)     # (N, 1, hidden size)\n",
    "emb_first_input.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_hidden = torch.cat((en_hidden[1], en_hidden[-1]), dim=-1).unsqueeze(1)    # shape (N, 1, decoder hidden size) \n",
    "encoder_final_hidden.device\n",
    "encoder_final_cell = torch.cat((en_cell[1], en_cell[-1]), dim=-1).unsqueeze(1)    # shape (N, 1, decoder hidden size)\n",
    "encoder_final_cell.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = F.scaled_dot_product_attention(encoder_final_hidden, en_output, en_output)\n",
    "context.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.cat((emb_first_input, context), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hidden = torch.rand((1, 4, 128)).to(device)\n",
    "test_cell = torch.rand((1, 4, 128)).to(device)\n",
    "test_input = torch.rand((4, 1, 256)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, (de_hidden, de_cell) = test_lstm(decoder_input, (encoder_final_hidden, encoder_final_cell))   # (N, 1, hidden size) lstmout\n",
    "lstm_out.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_output = test_fc(lstm_out)   # shape (N, 1, 27)  middle dimension is 1 because we are doing one letter at a time, so sequence length is 1\n",
    "fc_output.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_out = torch.rand((4, 1, 27)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for using when not teacher forcing\n",
    "predicted_tokens = torch.argmax(example_out.squeeze(1), dim=-1) # 1D with length N\n",
    "new_input = test_emb(predicted_tokens.unsqueeze(1)) # (N, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_targets = torch.randint(0,28, (4,)).to(device)  # (N) shape\n",
    "example_input = test_emb(example_targets.unsqueeze(1))\n",
    "example_targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(decoder_input, hidden, cell, encoder_output):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        decoder_input: shape (N), should be indicies for the letters, dtype either int or long\n",
    "        hidden: hidden state of lstm (D*Layers, N, decoder hidden)\n",
    "        cell: cell state of lstm (D*Layers, N, deocder hidden)\n",
    "        encoder_output: output from encoder (N, L, decoder hidden)\n",
    "    \"\"\"\n",
    "    embedded = test_emb(decoder_input.unsqueeze(1))  # (N, 1 decoder hidden)\n",
    "    query = hidden.permute(1, 0, 2) # (N, D*Layers, decoder hidden)\n",
    "    context = F.scaled_dot_product_attention(query, encoder_output, encoder_output) # (N, 1, decoder hidden)\n",
    "    input_lstm = torch.cat((embedded, context), dim=-1) # (N, 1, 2*decoder hidden)\n",
    "    output_lstm, (hidden, cell) = test_lstm(input_lstm, (hidden, cell)) # output lstm (N, 1, D*hidden)\n",
    "    output_fc = test_fc(output_lstm)\n",
    "\n",
    "    return output_fc, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(first_input.squeeze(1), encoder_final_hidden.permute(1, 0 , 2), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BLANK_TOKEN = 0\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size=27, force_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.force_ratio = force_ratio\n",
    "        self.lstm = nn.LSTM(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor):\n",
    "        # Ensure target_tensor is on the same device as the model\n",
    "        target_tensor = target_tensor.to(next(self.parameters()).device)\n",
    "        \n",
    "        # target tensor shape is [N, target length]\n",
    "        batch_size = target_tensor.size(0)\n",
    "        target_tensor = target_tensor.long()  # Ensure target_tensor is LongTensor\n",
    "        \n",
    "        # Initialize decoder_input with BLANK_TOKEN on the correct device, WRONNGNGNNGNGNGNNGNGNGNNGG\n",
    "        decoder_input = self.embedding(\n",
    "            torch.tensor([BLANK_TOKEN] * batch_size, dtype=torch.long, device=target_tensor.device).unsqueeze(1))  # Shape: (batch_size, 1, hidden_size)\n",
    "        \n",
    "        # Initial decoder hidden state is encoder hidden state\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        target_length = target_tensor.shape[1]\n",
    "\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell = self.forward_step(decoder_input.long(), decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            teacher_force = torch.rand(1).item() < self.force_ratio\n",
    "            teacher_force = True  # Force teacher forcing for debugging\n",
    "\n",
    "            if teacher_force:\n",
    "                # Use ground truth token\n",
    "                decoder_input = self.embedding(target_tensor[:, i].unsqueeze(1))  # Shape: (batch_size, 1, hidden_size)\n",
    "            else:\n",
    "                # Use predicted token\n",
    "                predicted_token = torch.argmax(decoder_output, dim=-1)\n",
    "                predicted_token = torch.clamp(predicted_token, 0, self.embedding.num_embeddings - 1)  # Clamp to valid range\n",
    "                decoder_input = self.embedding(predicted_token.unsqueeze(1))  # Shape: (batch_size, 1, hidden_size)\n",
    "\n",
    "        decoder_outputs = torch.stack(decoder_outputs, dim=1)  # Shape: (batch_size, sequence_length, output_size)\n",
    "        decoder_outputs = F.softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)  # Shape: (batch_size, 1, hidden_size)\n",
    "        query = hidden.permute(1, 0, 2)  # Shape: (batch_size, 1, hidden_size)\n",
    "        print(query)\n",
    "        context = F.scaled_dot_product_attention(query, encoder_outputs, encoder_outputs)  # Shape: (batch_size, 1, hidden_size)\n",
    "        decoder_input = torch.cat((embedded, context), dim=-1)  # Shape: (batch_size, 1, 2 * hidden_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(decoder_input, hidden)  # Shape: (batch_size, 1, hidden_size)\n",
    "        output = self.fc(lstm_out)  # Shape: (batch_size, 1, output_size)\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][2][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(hidden_size=64, output_size=27).to(device)\n",
    "out = decoder(en_output, en_hidden, data[0][2].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.embedding.num_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
