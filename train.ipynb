{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_torch_dataset import SwipeDataset\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset\")\n",
    "\n",
    "data = SwipeDataset(data_dir=dataset_path,\n",
    "                    batch=False)\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "reduced_set = Subset(data, range(200))\n",
    "train_set, val_set, test_set = random_split(reduced_set, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch: List of tuples (input, word, word_tensor)\n",
    "                  - input: (T, 6)\n",
    "                  - word: a string of characters\n",
    "                  - word_tensor: encoded word as indicies with 0 as the blank\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort batch by sequence length (descending order)\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "\n",
    "    inputs, words, targets = zip(*batch)\n",
    "    input_lengths = torch.LongTensor([x.shape[0] for x in inputs])  # store the lengths of inputs\n",
    "    input = pack_sequence(inputs)   # pack the inputs\n",
    "\n",
    "    target_lengths = torch.LongTensor([len(x) for x in words])\n",
    "    targets = torch.cat(targets)    # concatenate all the targets\n",
    "\n",
    "    return input, targets, input_lengths, target_lengths, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "dataloaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [x.shape[1] for x in data[:][0][:]]\n",
    "max(input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    # Pad inputs dynamically\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=(0, 0, -1, 0, 0, 0))\n",
    "    \n",
    "    # Pad targets dynamically (if needed)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=\"PAD\")\n",
    "    \n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_word, test_tensor = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Seq2Seq\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t_model = Seq2Seq(hidden_size=32,\n",
    "                  num_layers=2, \n",
    "                  input_size=6, \n",
    "                  output_size=27).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch_indicies, set, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Takes a batch of variable lengths\n",
    "    \"\"\"\n",
    "    for i in batch_indicies:\n",
    "        input, word, word_tensor = set[i]\n",
    "        input = input.to(device)\n",
    "        word_tensor = word_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_model(model, train_set, val_set, optimiser, criterion=nn.CTCLoss(), batch_size=32, num_epochs=10):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # set the mode of the model based on the phase and change the data used\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                batches = list(range(len(train_set)))\n",
    "                data = train_set\n",
    "            else:\n",
    "                model.eval()\n",
    "                batches = list(range(len(val_set)))\n",
    "                data = val_set\n",
    "\n",
    "            # batch like this because the words are of different lenghts and batching isnt used\n",
    "            random.shuffle(batches)\n",
    "            batches = np.array_split(batches, len(batches) // batch_size)\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for batch in batches:   # iterate over each batch of dataset\n",
    "                batch_loss = 0\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                # enable gradients only if in training mode\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    for i in batch:     # for every datapoint in the batch\n",
    "                        input, word, word_tensor = data[i]\n",
    "\n",
    "                        input = input.to(device)\n",
    "                        word_tensor = word_tensor.to(device)\n",
    "                        word_length = len(word)\n",
    "\n",
    "                        output = t_model(input)\n",
    "                        # rearrange the output for CTC loss\n",
    "                        output = output.permute(1, 0, 2)    # (T, N, C)\n",
    "                        # convert to tensors\n",
    "                        input_lengths = torch.LongTensor([input.shape[1]]).to(device)\n",
    "                        target_lengths = torch.LongTensor([word_length]).to(device)\n",
    "\n",
    "                        loss = criterion(output, word_tensor, input_lengths, target_lengths)\n",
    "                        batch_loss += loss\n",
    "                    # find mean batch loss\n",
    "                    avg_batch_loss = batch_loss / len(batch)\n",
    "\n",
    "                    # back prop only if in train\n",
    "                    if phase == \"train\":\n",
    "                        avg_batch_loss.backward()\n",
    "                        # clip the loss so we dont get exploding gradients\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                        optimiser.step()\n",
    "                # calculates the total loss for epoch over all batches\n",
    "                running_loss += avg_batch_loss.item()\n",
    "\n",
    "            # track epoch oss\n",
    "            if phase == \"train\":\n",
    "                epoch_loss = running_loss / len(batches)\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                epoch_loss = running_loss / len(batches)\n",
    "                val_losses.append(epoch_loss)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f\"Time elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")       \n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Seq2Seq\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t_model = Seq2Seq(hidden_size=32,\n",
    "                  num_layers=2, \n",
    "                  input_size=6, \n",
    "                  output_size=27).to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(t_model.parameters(), lr=0.01)\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "t_model = train_model(t_model, train_set, val_set, optimiser,criterion, num_epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_word, test_ten = test_set[0]\n",
    "test_in = test_in.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = t_model(test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'_': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8,\n",
    "              'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16,\n",
    "              'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24,\n",
    "              'y': 25, 'z': 26}\n",
    "reversed_vocab = {k: u for u, k in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outputs(decoder_output):\n",
    "    indicies = torch.argmax(decoder_output.squeeze(1), dim=-1).tolist()\n",
    "    words = []\n",
    "    for word in indicies:\n",
    "        characters = [reversed_vocab[i] for i in word]\n",
    "        words.append(characters)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outputs(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_sequence\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class CTCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, output_size=27, bidirectional=True):\n",
    "        super(CTCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=False,\n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size if bidirectional else hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        : param x: (batch_size, seq_len, input_size)\n",
    "        : return logits: (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        lstm_outputs, _ = self.lstm(x)  # lstm_out shape: (seq_len, batch_size, hidden_size*2 if bidirectional)\n",
    "\n",
    "        lstm_out = pad_packed_sequence(lstm_outputs, batch_first=False)[0]\n",
    "\n",
    "        logits = self.fc(lstm_out)  # shape: (batch_size, seq_len, output_size)\n",
    "        return F.log_softmax(logits, dim=-1)  # Log-softmax for CTC loss\n",
    "    \n",
    "c_model = CTCEncoder(input_size=6,\n",
    "                     hidden_size=32,\n",
    "                     num_layers=2,\n",
    "                     output_size=27,\n",
    "                     bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_inputs, t_targets, t_input_lengths, t_target_lengths, t_words = next(iter(train_loader))\n",
    "t_output = c_model(t_inputs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "\n",
    "def CTCtrain(model, dataloaders, optimiser, num_epochs = 5, scheduler = None):\n",
    "    criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        lowest_loss = float(\"Inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, targets, input_lengths, target_lengths, words in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    input_lengths = input_lengths.to(device)\n",
    "                    target_lengths = target_lengths.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimiser.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "                            loss.backward()\n",
    "                            optimiser.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * len(target_lengths)  # multiply by batch size\n",
    "                if phase == 'train' and scheduler is not None:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "                epoch_loss = running_loss / len(dataloaders[phase])\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_loss < lowest_loss:\n",
    "                    lowest_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Time Elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Lowest Loss: {lowest_loss:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "----------\n",
      "train Loss: 53.1365\n",
      "val Loss: 49.0688\n",
      "Time Elapsed: 0m 14s\n",
      "\n",
      "Epoch 2/300\n",
      "----------\n",
      "train Loss: 49.3247\n",
      "val Loss: 48.2561\n",
      "Time Elapsed: 0m 29s\n",
      "\n",
      "Epoch 3/300\n",
      "----------\n",
      "train Loss: 48.6487\n",
      "val Loss: 47.8550\n",
      "Time Elapsed: 0m 43s\n",
      "\n",
      "Epoch 4/300\n",
      "----------\n",
      "train Loss: 48.1371\n",
      "val Loss: 47.4451\n",
      "Time Elapsed: 0m 58s\n",
      "\n",
      "Epoch 5/300\n",
      "----------\n",
      "train Loss: 48.2068\n",
      "val Loss: 47.0347\n",
      "Time Elapsed: 1m 12s\n",
      "\n",
      "Epoch 6/300\n",
      "----------\n",
      "train Loss: 47.6476\n",
      "val Loss: 46.9267\n",
      "Time Elapsed: 1m 26s\n",
      "\n",
      "Epoch 7/300\n",
      "----------\n",
      "train Loss: 47.4838\n",
      "val Loss: 47.0585\n",
      "Time Elapsed: 1m 40s\n",
      "\n",
      "Epoch 8/300\n",
      "----------\n",
      "train Loss: 47.1763\n",
      "val Loss: 46.7381\n",
      "Time Elapsed: 1m 55s\n",
      "\n",
      "Epoch 9/300\n",
      "----------\n",
      "train Loss: 46.8663\n",
      "val Loss: 46.8102\n",
      "Time Elapsed: 2m 9s\n",
      "\n",
      "Epoch 10/300\n",
      "----------\n",
      "train Loss: 46.6139\n",
      "val Loss: 46.8531\n",
      "Time Elapsed: 2m 23s\n",
      "\n",
      "Epoch 11/300\n",
      "----------\n",
      "train Loss: 46.4473\n",
      "val Loss: 45.9656\n",
      "Time Elapsed: 2m 38s\n",
      "\n",
      "Epoch 12/300\n",
      "----------\n",
      "train Loss: 46.1274\n",
      "val Loss: 45.5698\n",
      "Time Elapsed: 2m 52s\n",
      "\n",
      "Epoch 13/300\n",
      "----------\n",
      "train Loss: 46.0027\n",
      "val Loss: 45.6680\n",
      "Time Elapsed: 3m 7s\n",
      "\n",
      "Epoch 14/300\n",
      "----------\n",
      "train Loss: 45.6885\n",
      "val Loss: 45.8261\n",
      "Time Elapsed: 3m 22s\n",
      "\n",
      "Epoch 15/300\n",
      "----------\n",
      "train Loss: 45.3779\n",
      "val Loss: 44.6674\n",
      "Time Elapsed: 3m 36s\n",
      "\n",
      "Epoch 16/300\n",
      "----------\n",
      "train Loss: 45.1097\n",
      "val Loss: 45.1229\n",
      "Time Elapsed: 3m 51s\n",
      "\n",
      "Epoch 17/300\n",
      "----------\n",
      "train Loss: 44.8732\n",
      "val Loss: 45.1909\n",
      "Time Elapsed: 4m 6s\n",
      "\n",
      "Epoch 18/300\n",
      "----------\n",
      "train Loss: 44.6761\n",
      "val Loss: 44.2400\n",
      "Time Elapsed: 4m 20s\n",
      "\n",
      "Epoch 19/300\n",
      "----------\n",
      "train Loss: 44.3135\n",
      "val Loss: 44.3485\n",
      "Time Elapsed: 4m 35s\n",
      "\n",
      "Epoch 20/300\n",
      "----------\n",
      "train Loss: 44.0562\n",
      "val Loss: 44.6238\n",
      "Time Elapsed: 4m 49s\n",
      "\n",
      "Epoch 21/300\n",
      "----------\n",
      "train Loss: 44.2074\n",
      "val Loss: 44.1765\n",
      "Time Elapsed: 5m 5s\n",
      "\n",
      "Epoch 22/300\n",
      "----------\n",
      "train Loss: 43.9574\n",
      "val Loss: 44.5409\n",
      "Time Elapsed: 5m 20s\n",
      "\n",
      "Epoch 23/300\n",
      "----------\n",
      "train Loss: 43.5405\n",
      "val Loss: 44.0583\n",
      "Time Elapsed: 5m 34s\n",
      "\n",
      "Epoch 24/300\n",
      "----------\n",
      "train Loss: 43.5695\n",
      "val Loss: 43.8911\n",
      "Time Elapsed: 5m 49s\n",
      "\n",
      "Epoch 25/300\n",
      "----------\n",
      "train Loss: 43.2001\n",
      "val Loss: 44.4929\n",
      "Time Elapsed: 6m 3s\n",
      "\n",
      "Epoch 26/300\n",
      "----------\n",
      "train Loss: 43.3719\n",
      "val Loss: 43.4022\n",
      "Time Elapsed: 6m 18s\n",
      "\n",
      "Epoch 27/300\n",
      "----------\n",
      "train Loss: 43.4903\n",
      "val Loss: 43.6103\n",
      "Time Elapsed: 6m 33s\n",
      "\n",
      "Epoch 28/300\n",
      "----------\n",
      "train Loss: 43.1200\n",
      "val Loss: 43.6477\n",
      "Time Elapsed: 6m 48s\n",
      "\n",
      "Epoch 29/300\n",
      "----------\n",
      "train Loss: 43.2289\n",
      "val Loss: 43.2130\n",
      "Time Elapsed: 7m 2s\n",
      "\n",
      "Epoch 30/300\n",
      "----------\n",
      "train Loss: 42.7920\n",
      "val Loss: 43.0914\n",
      "Time Elapsed: 7m 17s\n",
      "\n",
      "Epoch 31/300\n",
      "----------\n",
      "train Loss: 42.6057\n",
      "val Loss: 42.9020\n",
      "Time Elapsed: 7m 31s\n",
      "\n",
      "Epoch 32/300\n",
      "----------\n",
      "train Loss: 42.4528\n",
      "val Loss: 42.7441\n",
      "Time Elapsed: 7m 46s\n",
      "\n",
      "Epoch 33/300\n",
      "----------\n",
      "train Loss: 42.1954\n",
      "val Loss: 44.3842\n",
      "Time Elapsed: 8m 1s\n",
      "\n",
      "Epoch 34/300\n",
      "----------\n",
      "train Loss: 42.0385\n",
      "val Loss: 43.8891\n",
      "Time Elapsed: 8m 17s\n",
      "\n",
      "Epoch 35/300\n",
      "----------\n",
      "train Loss: 40.5796\n",
      "val Loss: 41.6965\n",
      "Time Elapsed: 8m 31s\n",
      "\n",
      "Epoch 36/300\n",
      "----------\n",
      "train Loss: 40.1983\n",
      "val Loss: 42.3393\n",
      "Time Elapsed: 8m 46s\n",
      "\n",
      "Epoch 37/300\n",
      "----------\n",
      "train Loss: 39.9196\n",
      "val Loss: 41.9227\n",
      "Time Elapsed: 9m 1s\n",
      "\n",
      "Epoch 38/300\n",
      "----------\n",
      "train Loss: 39.6419\n",
      "val Loss: 42.1627\n",
      "Time Elapsed: 9m 15s\n",
      "\n",
      "Epoch 39/300\n",
      "----------\n",
      "train Loss: 39.4078\n",
      "val Loss: 42.6010\n",
      "Time Elapsed: 9m 30s\n",
      "\n",
      "Epoch 40/300\n",
      "----------\n",
      "train Loss: 39.2496\n",
      "val Loss: 41.3667\n",
      "Time Elapsed: 9m 44s\n",
      "\n",
      "Epoch 41/300\n",
      "----------\n",
      "train Loss: 38.8579\n",
      "val Loss: 41.2478\n",
      "Time Elapsed: 9m 59s\n",
      "\n",
      "Epoch 42/300\n",
      "----------\n",
      "train Loss: 38.4366\n",
      "val Loss: 41.2391\n",
      "Time Elapsed: 10m 13s\n",
      "\n",
      "Epoch 43/300\n",
      "----------\n",
      "train Loss: 38.1140\n",
      "val Loss: 40.9449\n",
      "Time Elapsed: 10m 27s\n",
      "\n",
      "Epoch 44/300\n",
      "----------\n",
      "train Loss: 37.9263\n",
      "val Loss: 40.7513\n",
      "Time Elapsed: 10m 41s\n",
      "\n",
      "Epoch 45/300\n",
      "----------\n",
      "train Loss: 37.5765\n",
      "val Loss: 40.6689\n",
      "Time Elapsed: 10m 55s\n",
      "\n",
      "Epoch 46/300\n",
      "----------\n",
      "train Loss: 37.0626\n",
      "val Loss: 40.9145\n",
      "Time Elapsed: 11m 10s\n",
      "\n",
      "Epoch 47/300\n",
      "----------\n",
      "train Loss: 36.7169\n",
      "val Loss: 40.2833\n",
      "Time Elapsed: 11m 24s\n",
      "\n",
      "Epoch 48/300\n",
      "----------\n",
      "train Loss: 36.4464\n",
      "val Loss: 41.1525\n",
      "Time Elapsed: 11m 39s\n",
      "\n",
      "Epoch 49/300\n",
      "----------\n",
      "train Loss: 34.8351\n",
      "val Loss: 40.2872\n",
      "Time Elapsed: 11m 53s\n",
      "\n",
      "Epoch 50/300\n",
      "----------\n",
      "train Loss: 34.3118\n",
      "val Loss: 40.0383\n",
      "Time Elapsed: 12m 7s\n",
      "\n",
      "Epoch 51/300\n",
      "----------\n",
      "train Loss: 33.9698\n",
      "val Loss: 40.3126\n",
      "Time Elapsed: 12m 21s\n",
      "\n",
      "Epoch 52/300\n",
      "----------\n",
      "train Loss: 33.4372\n",
      "val Loss: 40.4183\n",
      "Time Elapsed: 12m 35s\n",
      "\n",
      "Epoch 53/300\n",
      "----------\n",
      "train Loss: 32.9816\n",
      "val Loss: 39.7415\n",
      "Time Elapsed: 12m 50s\n",
      "\n",
      "Epoch 54/300\n",
      "----------\n",
      "train Loss: 32.6262\n",
      "val Loss: 39.6551\n",
      "Time Elapsed: 13m 4s\n",
      "\n",
      "Epoch 55/300\n",
      "----------\n",
      "train Loss: 32.1518\n",
      "val Loss: 40.0822\n",
      "Time Elapsed: 13m 18s\n",
      "\n",
      "Epoch 56/300\n",
      "----------\n",
      "train Loss: 31.8192\n",
      "val Loss: 39.2821\n",
      "Time Elapsed: 13m 32s\n",
      "\n",
      "Epoch 57/300\n",
      "----------\n",
      "train Loss: 31.2655\n",
      "val Loss: 40.1138\n",
      "Time Elapsed: 13m 46s\n",
      "\n",
      "Epoch 58/300\n",
      "----------\n",
      "train Loss: 30.9464\n",
      "val Loss: 39.8132\n",
      "Time Elapsed: 14m 1s\n",
      "\n",
      "Epoch 59/300\n",
      "----------\n",
      "train Loss: 30.5396\n",
      "val Loss: 39.4760\n",
      "Time Elapsed: 14m 16s\n",
      "\n",
      "Epoch 60/300\n",
      "----------\n",
      "train Loss: 28.8716\n",
      "val Loss: 39.4944\n",
      "Time Elapsed: 14m 30s\n",
      "\n",
      "Epoch 61/300\n",
      "----------\n",
      "train Loss: 28.5111\n",
      "val Loss: 39.4743\n",
      "Time Elapsed: 14m 44s\n",
      "\n",
      "Epoch 62/300\n",
      "----------\n",
      "train Loss: 28.0068\n",
      "val Loss: 40.0986\n",
      "Time Elapsed: 14m 58s\n",
      "\n",
      "Epoch 63/300\n",
      "----------\n",
      "train Loss: 27.7089\n",
      "val Loss: 40.0537\n",
      "Time Elapsed: 15m 12s\n",
      "\n",
      "Epoch 64/300\n",
      "----------\n",
      "train Loss: 27.3631\n",
      "val Loss: 39.9555\n",
      "Time Elapsed: 15m 26s\n",
      "\n",
      "Epoch 65/300\n",
      "----------\n",
      "train Loss: 26.9843\n",
      "val Loss: 40.4025\n",
      "Time Elapsed: 15m 41s\n",
      "\n",
      "Epoch 66/300\n",
      "----------\n",
      "train Loss: 26.6116\n",
      "val Loss: 40.3554\n",
      "Time Elapsed: 15m 55s\n",
      "\n",
      "Epoch 67/300\n",
      "----------\n",
      "train Loss: 26.3086\n",
      "val Loss: 40.3798\n",
      "Time Elapsed: 16m 9s\n",
      "\n",
      "Epoch 68/300\n",
      "----------\n",
      "train Loss: 25.9674\n",
      "val Loss: 40.6011\n",
      "Time Elapsed: 16m 23s\n",
      "\n",
      "Epoch 69/300\n",
      "----------\n",
      "train Loss: 25.5836\n",
      "val Loss: 40.9242\n",
      "Time Elapsed: 16m 37s\n",
      "\n",
      "Epoch 70/300\n",
      "----------\n",
      "train Loss: 25.1553\n",
      "val Loss: 40.5329\n",
      "Time Elapsed: 16m 51s\n",
      "\n",
      "Epoch 71/300\n",
      "----------\n",
      "train Loss: 24.9183\n",
      "val Loss: 40.7121\n",
      "Time Elapsed: 17m 6s\n",
      "\n",
      "Epoch 72/300\n",
      "----------\n",
      "train Loss: 24.4994\n",
      "val Loss: 41.5711\n",
      "Time Elapsed: 17m 20s\n",
      "\n",
      "Epoch 73/300\n",
      "----------\n",
      "train Loss: 25.0818\n",
      "val Loss: 41.5777\n",
      "Time Elapsed: 17m 35s\n",
      "\n",
      "Epoch 74/300\n",
      "----------\n",
      "train Loss: 23.9267\n",
      "val Loss: 41.7508\n",
      "Time Elapsed: 17m 49s\n",
      "\n",
      "Epoch 75/300\n",
      "----------\n",
      "train Loss: 23.6516\n",
      "val Loss: 41.5933\n",
      "Time Elapsed: 18m 2s\n",
      "\n",
      "Epoch 76/300\n",
      "----------\n",
      "train Loss: 23.5137\n",
      "val Loss: 41.9240\n",
      "Time Elapsed: 18m 17s\n",
      "\n",
      "Epoch 77/300\n",
      "----------\n",
      "train Loss: 22.7686\n",
      "val Loss: 41.5621\n",
      "Time Elapsed: 18m 31s\n",
      "\n",
      "Epoch 78/300\n",
      "----------\n",
      "train Loss: 22.6325\n",
      "val Loss: 42.3187\n",
      "Time Elapsed: 18m 45s\n",
      "\n",
      "Epoch 79/300\n",
      "----------\n",
      "train Loss: 22.1011\n",
      "val Loss: 43.8654\n",
      "Time Elapsed: 18m 59s\n",
      "\n",
      "Epoch 80/300\n",
      "----------\n",
      "train Loss: 20.8906\n",
      "val Loss: 42.9402\n",
      "Time Elapsed: 19m 13s\n",
      "\n",
      "Epoch 81/300\n",
      "----------\n",
      "train Loss: 20.4379\n",
      "val Loss: 43.2441\n",
      "Time Elapsed: 19m 28s\n",
      "\n",
      "Epoch 82/300\n",
      "----------\n",
      "train Loss: 20.1681\n",
      "val Loss: 43.5821\n",
      "Time Elapsed: 19m 42s\n",
      "\n",
      "Epoch 83/300\n",
      "----------\n",
      "train Loss: 19.9580\n",
      "val Loss: 43.4699\n",
      "Time Elapsed: 19m 56s\n",
      "\n",
      "Epoch 84/300\n",
      "----------\n",
      "train Loss: 19.6578\n",
      "val Loss: 44.0134\n",
      "Time Elapsed: 20m 11s\n",
      "\n",
      "Epoch 85/300\n",
      "----------\n",
      "train Loss: 19.4381\n",
      "val Loss: 43.8170\n",
      "Time Elapsed: 20m 25s\n",
      "\n",
      "Epoch 86/300\n",
      "----------\n",
      "train Loss: 19.0848\n",
      "val Loss: 44.4793\n",
      "Time Elapsed: 20m 40s\n",
      "\n",
      "Epoch 87/300\n",
      "----------\n",
      "train Loss: 19.0217\n",
      "val Loss: 44.4952\n",
      "Time Elapsed: 20m 54s\n",
      "\n",
      "Epoch 88/300\n",
      "----------\n",
      "train Loss: 18.7448\n",
      "val Loss: 44.5500\n",
      "Time Elapsed: 21m 8s\n",
      "\n",
      "Epoch 89/300\n",
      "----------\n",
      "train Loss: 18.4845\n",
      "val Loss: 44.7585\n",
      "Time Elapsed: 21m 22s\n",
      "\n",
      "Epoch 90/300\n",
      "----------\n",
      "train Loss: 18.2971\n",
      "val Loss: 45.1769\n",
      "Time Elapsed: 21m 36s\n",
      "\n",
      "Epoch 91/300\n",
      "----------\n",
      "train Loss: 18.0058\n",
      "val Loss: 45.8759\n",
      "Time Elapsed: 21m 50s\n",
      "\n",
      "Epoch 92/300\n",
      "----------\n",
      "train Loss: 17.9027\n",
      "val Loss: 45.7091\n",
      "Time Elapsed: 22m 4s\n",
      "\n",
      "Epoch 93/300\n",
      "----------\n",
      "train Loss: 17.5675\n",
      "val Loss: 45.7239\n",
      "Time Elapsed: 22m 18s\n",
      "\n",
      "Epoch 94/300\n",
      "----------\n",
      "train Loss: 17.3449\n",
      "val Loss: 46.2487\n",
      "Time Elapsed: 22m 32s\n",
      "\n",
      "Epoch 95/300\n",
      "----------\n",
      "train Loss: 17.1335\n",
      "val Loss: 46.5527\n",
      "Time Elapsed: 22m 46s\n",
      "\n",
      "Epoch 96/300\n",
      "----------\n",
      "train Loss: 16.9348\n",
      "val Loss: 46.4685\n",
      "Time Elapsed: 23m 1s\n",
      "\n",
      "Epoch 97/300\n",
      "----------\n",
      "train Loss: 16.8272\n",
      "val Loss: 46.5606\n",
      "Time Elapsed: 23m 16s\n",
      "\n",
      "Epoch 98/300\n",
      "----------\n",
      "train Loss: 16.6519\n",
      "val Loss: 46.8569\n",
      "Time Elapsed: 23m 30s\n",
      "\n",
      "Epoch 99/300\n",
      "----------\n",
      "train Loss: 16.3247\n",
      "val Loss: 47.6458\n",
      "Time Elapsed: 23m 44s\n",
      "\n",
      "Epoch 100/300\n",
      "----------\n",
      "train Loss: 16.0897\n",
      "val Loss: 47.3462\n",
      "Time Elapsed: 23m 59s\n",
      "\n",
      "Epoch 101/300\n",
      "----------\n",
      "train Loss: 15.3098\n",
      "val Loss: 47.8158\n",
      "Time Elapsed: 24m 12s\n",
      "\n",
      "Epoch 102/300\n",
      "----------\n",
      "train Loss: 15.2161\n",
      "val Loss: 48.0259\n",
      "Time Elapsed: 24m 27s\n",
      "\n",
      "Epoch 103/300\n",
      "----------\n",
      "train Loss: 14.9621\n",
      "val Loss: 48.5076\n",
      "Time Elapsed: 24m 41s\n",
      "\n",
      "Epoch 104/300\n",
      "----------\n",
      "train Loss: 14.8116\n",
      "val Loss: 48.9477\n",
      "Time Elapsed: 24m 55s\n",
      "\n",
      "Epoch 105/300\n",
      "----------\n",
      "train Loss: 14.6771\n",
      "val Loss: 49.3759\n",
      "Time Elapsed: 25m 9s\n",
      "\n",
      "Epoch 106/300\n",
      "----------\n",
      "train Loss: 14.5740\n",
      "val Loss: 48.8991\n",
      "Time Elapsed: 25m 23s\n",
      "\n",
      "Epoch 107/300\n",
      "----------\n",
      "train Loss: 14.4173\n",
      "val Loss: 49.5116\n",
      "Time Elapsed: 25m 37s\n",
      "\n",
      "Epoch 108/300\n",
      "----------\n",
      "train Loss: 14.2543\n",
      "val Loss: 49.8623\n",
      "Time Elapsed: 25m 52s\n",
      "\n",
      "Epoch 109/300\n",
      "----------\n",
      "train Loss: 14.1954\n",
      "val Loss: 49.8999\n",
      "Time Elapsed: 26m 6s\n",
      "\n",
      "Epoch 110/300\n",
      "----------\n",
      "train Loss: 14.0590\n",
      "val Loss: 49.9166\n",
      "Time Elapsed: 26m 21s\n",
      "\n",
      "Epoch 111/300\n",
      "----------\n",
      "train Loss: 13.8793\n",
      "val Loss: 50.3027\n",
      "Time Elapsed: 26m 35s\n",
      "\n",
      "Epoch 112/300\n",
      "----------\n",
      "train Loss: 13.7803\n",
      "val Loss: 50.1705\n",
      "Time Elapsed: 26m 49s\n",
      "\n",
      "Epoch 113/300\n",
      "----------\n",
      "train Loss: 13.7135\n",
      "val Loss: 50.5084\n",
      "Time Elapsed: 27m 3s\n",
      "\n",
      "Epoch 114/300\n",
      "----------\n",
      "train Loss: 13.5534\n",
      "val Loss: 50.7053\n",
      "Time Elapsed: 27m 17s\n",
      "\n",
      "Epoch 115/300\n",
      "----------\n",
      "train Loss: 13.0654\n",
      "val Loss: 50.9576\n",
      "Time Elapsed: 27m 31s\n",
      "\n",
      "Epoch 116/300\n",
      "----------\n",
      "train Loss: 12.9480\n",
      "val Loss: 51.3711\n",
      "Time Elapsed: 27m 45s\n",
      "\n",
      "Epoch 117/300\n",
      "----------\n",
      "train Loss: 12.8809\n",
      "val Loss: 51.1206\n",
      "Time Elapsed: 28m 0s\n",
      "\n",
      "Epoch 118/300\n",
      "----------\n",
      "train Loss: 12.7953\n",
      "val Loss: 51.4849\n",
      "Time Elapsed: 28m 14s\n",
      "\n",
      "Epoch 119/300\n",
      "----------\n",
      "train Loss: 12.7179\n",
      "val Loss: 51.4269\n",
      "Time Elapsed: 28m 28s\n",
      "\n",
      "Epoch 120/300\n",
      "----------\n",
      "train Loss: 12.6519\n",
      "val Loss: 51.6919\n",
      "Time Elapsed: 28m 43s\n",
      "\n",
      "Epoch 121/300\n",
      "----------\n",
      "train Loss: 12.5645\n",
      "val Loss: 52.0593\n",
      "Time Elapsed: 28m 57s\n",
      "\n",
      "Epoch 122/300\n",
      "----------\n",
      "train Loss: 12.4974\n",
      "val Loss: 51.9420\n",
      "Time Elapsed: 29m 13s\n",
      "\n",
      "Epoch 123/300\n",
      "----------\n",
      "train Loss: 12.4319\n",
      "val Loss: 51.9511\n",
      "Time Elapsed: 29m 27s\n",
      "\n",
      "Epoch 124/300\n",
      "----------\n",
      "train Loss: 12.3464\n",
      "val Loss: 52.3505\n",
      "Time Elapsed: 29m 41s\n",
      "\n",
      "Epoch 125/300\n",
      "----------\n",
      "train Loss: 12.2651\n",
      "val Loss: 52.4105\n",
      "Time Elapsed: 29m 55s\n",
      "\n",
      "Epoch 126/300\n",
      "----------\n",
      "train Loss: 12.2034\n",
      "val Loss: 52.4557\n",
      "Time Elapsed: 30m 9s\n",
      "\n",
      "Epoch 127/300\n",
      "----------\n",
      "train Loss: 12.1296\n",
      "val Loss: 52.5999\n",
      "Time Elapsed: 30m 24s\n",
      "\n",
      "Epoch 128/300\n",
      "----------\n",
      "train Loss: 12.0454\n",
      "val Loss: 52.7225\n",
      "Time Elapsed: 30m 38s\n",
      "\n",
      "Epoch 129/300\n",
      "----------\n",
      "train Loss: 11.9905\n",
      "val Loss: 52.9342\n",
      "Time Elapsed: 30m 52s\n",
      "\n",
      "Epoch 130/300\n",
      "----------\n",
      "train Loss: 11.9177\n",
      "val Loss: 52.8997\n",
      "Time Elapsed: 31m 5s\n",
      "\n",
      "Epoch 131/300\n",
      "----------\n",
      "train Loss: 11.8450\n",
      "val Loss: 52.6825\n",
      "Time Elapsed: 31m 20s\n",
      "\n",
      "Epoch 132/300\n",
      "----------\n",
      "train Loss: 11.7926\n",
      "val Loss: 53.1317\n",
      "Time Elapsed: 31m 34s\n",
      "\n",
      "Epoch 133/300\n",
      "----------\n",
      "train Loss: 11.6834\n",
      "val Loss: 53.5167\n",
      "Time Elapsed: 31m 48s\n",
      "\n",
      "Epoch 134/300\n",
      "----------\n",
      "train Loss: 11.6339\n",
      "val Loss: 53.7132\n",
      "Time Elapsed: 32m 2s\n",
      "\n",
      "Epoch 135/300\n",
      "----------\n",
      "train Loss: 11.5564\n",
      "val Loss: 53.7318\n",
      "Time Elapsed: 32m 17s\n",
      "\n",
      "Epoch 136/300\n",
      "----------\n",
      "train Loss: 11.4877\n",
      "val Loss: 53.7952\n",
      "Time Elapsed: 32m 31s\n",
      "\n",
      "Epoch 137/300\n",
      "----------\n",
      "train Loss: 11.4092\n",
      "val Loss: 53.9839\n",
      "Time Elapsed: 32m 45s\n",
      "\n",
      "Epoch 138/300\n",
      "----------\n",
      "train Loss: 11.3550\n",
      "val Loss: 54.1497\n",
      "Time Elapsed: 32m 60s\n",
      "\n",
      "Epoch 139/300\n",
      "----------\n",
      "train Loss: 11.1204\n",
      "val Loss: 54.2025\n",
      "Time Elapsed: 33m 14s\n",
      "\n",
      "Epoch 140/300\n",
      "----------\n",
      "train Loss: 11.0533\n",
      "val Loss: 54.2199\n",
      "Time Elapsed: 33m 28s\n",
      "\n",
      "Epoch 141/300\n",
      "----------\n",
      "train Loss: 11.0056\n",
      "val Loss: 54.4241\n",
      "Time Elapsed: 33m 42s\n",
      "\n",
      "Epoch 142/300\n",
      "----------\n",
      "train Loss: 10.9720\n",
      "val Loss: 54.4520\n",
      "Time Elapsed: 33m 56s\n",
      "\n",
      "Epoch 143/300\n",
      "----------\n",
      "train Loss: 10.9286\n",
      "val Loss: 54.6143\n",
      "Time Elapsed: 34m 10s\n",
      "\n",
      "Epoch 144/300\n",
      "----------\n",
      "train Loss: 10.8878\n",
      "val Loss: 54.7560\n",
      "Time Elapsed: 34m 24s\n",
      "\n",
      "Epoch 145/300\n",
      "----------\n",
      "train Loss: 10.8436\n",
      "val Loss: 54.9806\n",
      "Time Elapsed: 34m 39s\n",
      "\n",
      "Epoch 146/300\n",
      "----------\n",
      "train Loss: 10.8227\n",
      "val Loss: 54.7976\n",
      "Time Elapsed: 34m 53s\n",
      "\n",
      "Epoch 147/300\n",
      "----------\n",
      "train Loss: 10.7704\n",
      "val Loss: 55.1522\n",
      "Time Elapsed: 35m 7s\n",
      "\n",
      "Epoch 148/300\n",
      "----------\n",
      "train Loss: 10.7210\n",
      "val Loss: 55.2705\n",
      "Time Elapsed: 35m 22s\n",
      "\n",
      "Epoch 149/300\n",
      "----------\n",
      "train Loss: 10.7093\n",
      "val Loss: 55.1101\n",
      "Time Elapsed: 35m 36s\n",
      "\n",
      "Epoch 150/300\n",
      "----------\n",
      "train Loss: 10.6550\n",
      "val Loss: 55.1066\n",
      "Time Elapsed: 35m 50s\n",
      "\n",
      "Epoch 151/300\n",
      "----------\n",
      "train Loss: 10.6100\n",
      "val Loss: 55.4020\n",
      "Time Elapsed: 36m 4s\n",
      "\n",
      "Epoch 152/300\n",
      "----------\n",
      "train Loss: 10.4863\n",
      "val Loss: 55.5486\n",
      "Time Elapsed: 36m 18s\n",
      "\n",
      "Epoch 153/300\n",
      "----------\n",
      "train Loss: 10.4605\n",
      "val Loss: 55.6405\n",
      "Time Elapsed: 36m 32s\n",
      "\n",
      "Epoch 154/300\n",
      "----------\n",
      "train Loss: 10.4335\n",
      "val Loss: 55.6078\n",
      "Time Elapsed: 36m 47s\n",
      "\n",
      "Epoch 155/300\n",
      "----------\n",
      "train Loss: 10.4144\n",
      "val Loss: 55.6531\n",
      "Time Elapsed: 37m 1s\n",
      "\n",
      "Epoch 156/300\n",
      "----------\n",
      "train Loss: 10.3963\n",
      "val Loss: 55.6166\n",
      "Time Elapsed: 37m 15s\n",
      "\n",
      "Epoch 157/300\n",
      "----------\n",
      "train Loss: 10.3716\n",
      "val Loss: 55.7991\n",
      "Time Elapsed: 37m 29s\n",
      "\n",
      "Epoch 158/300\n",
      "----------\n",
      "train Loss: 10.3512\n",
      "val Loss: 55.7234\n",
      "Time Elapsed: 37m 43s\n",
      "\n",
      "Epoch 159/300\n",
      "----------\n",
      "train Loss: 10.3272\n",
      "val Loss: 55.8025\n",
      "Time Elapsed: 37m 57s\n",
      "\n",
      "Epoch 160/300\n",
      "----------\n",
      "train Loss: 10.3093\n",
      "val Loss: 55.9044\n",
      "Time Elapsed: 38m 11s\n",
      "\n",
      "Epoch 161/300\n",
      "----------\n",
      "train Loss: 10.2860\n",
      "val Loss: 56.0018\n",
      "Time Elapsed: 38m 26s\n",
      "\n",
      "Epoch 162/300\n",
      "----------\n",
      "train Loss: 10.2695\n",
      "val Loss: 55.9471\n",
      "Time Elapsed: 38m 40s\n",
      "\n",
      "Epoch 163/300\n",
      "----------\n",
      "train Loss: 10.2034\n",
      "val Loss: 55.9436\n",
      "Time Elapsed: 38m 54s\n",
      "\n",
      "Epoch 164/300\n",
      "----------\n",
      "train Loss: 10.1887\n",
      "val Loss: 56.0223\n",
      "Time Elapsed: 39m 8s\n",
      "\n",
      "Epoch 165/300\n",
      "----------\n",
      "train Loss: 10.1730\n",
      "val Loss: 56.0306\n",
      "Time Elapsed: 39m 22s\n",
      "\n",
      "Epoch 166/300\n",
      "----------\n",
      "train Loss: 10.1656\n",
      "val Loss: 56.0817\n",
      "Time Elapsed: 39m 36s\n",
      "\n",
      "Epoch 167/300\n",
      "----------\n",
      "train Loss: 10.1542\n",
      "val Loss: 56.0750\n",
      "Time Elapsed: 39m 50s\n",
      "\n",
      "Epoch 168/300\n",
      "----------\n",
      "train Loss: 10.1427\n",
      "val Loss: 56.1653\n",
      "Time Elapsed: 40m 5s\n",
      "\n",
      "Epoch 169/300\n",
      "----------\n",
      "train Loss: 10.1291\n",
      "val Loss: 56.1186\n",
      "Time Elapsed: 40m 19s\n",
      "\n",
      "Epoch 170/300\n",
      "----------\n",
      "train Loss: 10.1189\n",
      "val Loss: 56.1919\n",
      "Time Elapsed: 40m 33s\n",
      "\n",
      "Epoch 171/300\n",
      "----------\n",
      "train Loss: 10.1065\n",
      "val Loss: 56.2224\n",
      "Time Elapsed: 40m 47s\n",
      "\n",
      "Epoch 172/300\n",
      "----------\n",
      "train Loss: 10.0965\n",
      "val Loss: 56.2505\n",
      "Time Elapsed: 41m 1s\n",
      "\n",
      "Epoch 173/300\n",
      "----------\n",
      "train Loss: 10.0857\n",
      "val Loss: 56.3223\n",
      "Time Elapsed: 41m 16s\n",
      "\n",
      "Epoch 174/300\n",
      "----------\n",
      "train Loss: 10.0493\n",
      "val Loss: 56.2917\n",
      "Time Elapsed: 41m 31s\n",
      "\n",
      "Epoch 175/300\n",
      "----------\n",
      "train Loss: 10.0418\n",
      "val Loss: 56.3405\n",
      "Time Elapsed: 41m 45s\n",
      "\n",
      "Epoch 176/300\n",
      "----------\n",
      "train Loss: 10.0363\n",
      "val Loss: 56.3242\n",
      "Time Elapsed: 41m 59s\n",
      "\n",
      "Epoch 177/300\n",
      "----------\n",
      "train Loss: 10.0299\n",
      "val Loss: 56.3365\n",
      "Time Elapsed: 42m 13s\n",
      "\n",
      "Epoch 178/300\n",
      "----------\n",
      "train Loss: 10.0256\n",
      "val Loss: 56.3044\n",
      "Time Elapsed: 42m 27s\n",
      "\n",
      "Epoch 179/300\n",
      "----------\n",
      "train Loss: 10.0193\n",
      "val Loss: 56.3470\n",
      "Time Elapsed: 42m 41s\n",
      "\n",
      "Epoch 180/300\n",
      "----------\n",
      "train Loss: 10.0133\n",
      "val Loss: 56.3861\n",
      "Time Elapsed: 42m 55s\n",
      "\n",
      "Epoch 181/300\n",
      "----------\n",
      "train Loss: 10.0070\n",
      "val Loss: 56.3836\n",
      "Time Elapsed: 43m 9s\n",
      "\n",
      "Epoch 182/300\n",
      "----------\n",
      "train Loss: 10.0014\n",
      "val Loss: 56.4150\n",
      "Time Elapsed: 43m 23s\n",
      "\n",
      "Epoch 183/300\n",
      "----------\n",
      "train Loss: 9.9950\n",
      "val Loss: 56.4599\n",
      "Time Elapsed: 43m 37s\n",
      "\n",
      "Epoch 184/300\n",
      "----------\n",
      "train Loss: 9.9901\n",
      "val Loss: 56.4525\n",
      "Time Elapsed: 43m 51s\n",
      "\n",
      "Epoch 185/300\n",
      "----------\n",
      "train Loss: 9.9716\n",
      "val Loss: 56.4285\n",
      "Time Elapsed: 44m 5s\n",
      "\n",
      "Epoch 186/300\n",
      "----------\n",
      "train Loss: 9.9690\n",
      "val Loss: 56.4414\n",
      "Time Elapsed: 44m 20s\n",
      "\n",
      "Epoch 187/300\n",
      "----------\n",
      "train Loss: 9.9663\n",
      "val Loss: 56.4541\n",
      "Time Elapsed: 44m 34s\n",
      "\n",
      "Epoch 188/300\n",
      "----------\n",
      "train Loss: 9.9630\n",
      "val Loss: 56.4555\n",
      "Time Elapsed: 44m 48s\n",
      "\n",
      "Epoch 189/300\n",
      "----------\n",
      "train Loss: 9.9606\n",
      "val Loss: 56.4524\n",
      "Time Elapsed: 45m 3s\n",
      "\n",
      "Epoch 190/300\n",
      "----------\n",
      "train Loss: 9.9575\n",
      "val Loss: 56.4682\n",
      "Time Elapsed: 45m 17s\n",
      "\n",
      "Epoch 191/300\n",
      "----------\n",
      "train Loss: 9.9553\n",
      "val Loss: 56.4807\n",
      "Time Elapsed: 45m 31s\n",
      "\n",
      "Epoch 192/300\n",
      "----------\n",
      "train Loss: 9.9520\n",
      "val Loss: 56.4902\n",
      "Time Elapsed: 45m 45s\n",
      "\n",
      "Epoch 193/300\n",
      "----------\n",
      "train Loss: 9.9501\n",
      "val Loss: 56.5026\n",
      "Time Elapsed: 45m 59s\n",
      "\n",
      "Epoch 194/300\n",
      "----------\n",
      "train Loss: 9.9464\n",
      "val Loss: 56.4715\n",
      "Time Elapsed: 46m 13s\n",
      "\n",
      "Epoch 195/300\n",
      "----------\n",
      "train Loss: 9.9437\n",
      "val Loss: 56.5086\n",
      "Time Elapsed: 46m 27s\n",
      "\n",
      "Epoch 196/300\n",
      "----------\n",
      "train Loss: 9.9414\n",
      "val Loss: 56.5192\n",
      "Time Elapsed: 46m 41s\n",
      "\n",
      "Epoch 197/300\n",
      "----------\n",
      "train Loss: 9.9376\n",
      "val Loss: 56.5185\n",
      "Time Elapsed: 46m 55s\n",
      "\n",
      "Epoch 198/300\n",
      "----------\n",
      "train Loss: 9.9357\n",
      "val Loss: 56.5341\n",
      "Time Elapsed: 47m 9s\n",
      "\n",
      "Epoch 199/300\n",
      "----------\n",
      "train Loss: 9.9325\n",
      "val Loss: 56.5231\n",
      "Time Elapsed: 47m 24s\n",
      "\n",
      "Epoch 200/300\n",
      "----------\n",
      "train Loss: 9.9296\n",
      "val Loss: 56.5394\n",
      "Time Elapsed: 47m 39s\n",
      "\n",
      "Epoch 201/300\n",
      "----------\n",
      "train Loss: 9.9271\n",
      "val Loss: 56.5317\n",
      "Time Elapsed: 47m 54s\n",
      "\n",
      "Epoch 202/300\n",
      "----------\n",
      "train Loss: 9.9238\n",
      "val Loss: 56.5404\n",
      "Time Elapsed: 48m 8s\n",
      "\n",
      "Epoch 203/300\n",
      "----------\n",
      "train Loss: 9.9210\n",
      "val Loss: 56.5558\n",
      "Time Elapsed: 48m 22s\n",
      "\n",
      "Epoch 204/300\n",
      "----------\n",
      "train Loss: 9.9122\n",
      "val Loss: 56.5525\n",
      "Time Elapsed: 48m 36s\n",
      "\n",
      "Epoch 205/300\n",
      "----------\n",
      "train Loss: 9.9103\n",
      "val Loss: 56.5582\n",
      "Time Elapsed: 48m 51s\n",
      "\n",
      "Epoch 206/300\n",
      "----------\n",
      "train Loss: 9.9084\n",
      "val Loss: 56.5633\n",
      "Time Elapsed: 49m 5s\n",
      "\n",
      "Epoch 207/300\n",
      "----------\n",
      "train Loss: 9.9072\n",
      "val Loss: 56.5775\n",
      "Time Elapsed: 49m 19s\n",
      "\n",
      "Epoch 208/300\n",
      "----------\n",
      "train Loss: 9.9055\n",
      "val Loss: 56.5712\n",
      "Time Elapsed: 49m 33s\n",
      "\n",
      "Epoch 209/300\n",
      "----------\n",
      "train Loss: 9.9034\n",
      "val Loss: 56.5815\n",
      "Time Elapsed: 49m 47s\n",
      "\n",
      "Epoch 210/300\n",
      "----------\n",
      "train Loss: 9.9018\n",
      "val Loss: 56.5790\n",
      "Time Elapsed: 50m 1s\n",
      "\n",
      "Epoch 211/300\n",
      "----------\n",
      "train Loss: 9.9008\n",
      "val Loss: 56.5763\n",
      "Time Elapsed: 50m 16s\n",
      "\n",
      "Epoch 212/300\n",
      "----------\n",
      "train Loss: 9.8989\n",
      "val Loss: 56.5988\n",
      "Time Elapsed: 50m 30s\n",
      "\n",
      "Epoch 213/300\n",
      "----------\n",
      "train Loss: 9.8972\n",
      "val Loss: 56.5923\n",
      "Time Elapsed: 50m 44s\n",
      "\n",
      "Epoch 214/300\n",
      "----------\n",
      "train Loss: 9.8959\n",
      "val Loss: 56.5940\n",
      "Time Elapsed: 50m 59s\n",
      "\n",
      "Epoch 215/300\n",
      "----------\n",
      "train Loss: 9.8912\n",
      "val Loss: 56.6012\n",
      "Time Elapsed: 51m 13s\n",
      "\n",
      "Epoch 216/300\n",
      "----------\n",
      "train Loss: 9.8899\n",
      "val Loss: 56.5971\n",
      "Time Elapsed: 51m 27s\n",
      "\n",
      "Epoch 217/300\n",
      "----------\n",
      "train Loss: 9.8892\n",
      "val Loss: 56.6027\n",
      "Time Elapsed: 51m 41s\n",
      "\n",
      "Epoch 218/300\n",
      "----------\n",
      "train Loss: 9.8884\n",
      "val Loss: 56.6089\n",
      "Time Elapsed: 51m 55s\n",
      "\n",
      "Epoch 219/300\n",
      "----------\n",
      "train Loss: 9.8875\n",
      "val Loss: 56.6096\n",
      "Time Elapsed: 52m 10s\n",
      "\n",
      "Epoch 220/300\n",
      "----------\n",
      "train Loss: 9.8868\n",
      "val Loss: 56.6031\n",
      "Time Elapsed: 52m 24s\n",
      "\n",
      "Epoch 221/300\n",
      "----------\n",
      "train Loss: 9.8860\n",
      "val Loss: 56.6116\n",
      "Time Elapsed: 52m 37s\n",
      "\n",
      "Epoch 222/300\n",
      "----------\n",
      "train Loss: 9.8852\n",
      "val Loss: 56.6121\n",
      "Time Elapsed: 52m 51s\n",
      "\n",
      "Epoch 223/300\n",
      "----------\n",
      "train Loss: 9.8843\n",
      "val Loss: 56.6206\n",
      "Time Elapsed: 53m 5s\n",
      "\n",
      "Epoch 224/300\n",
      "----------\n",
      "train Loss: 9.8838\n",
      "val Loss: 56.6189\n",
      "Time Elapsed: 53m 21s\n",
      "\n",
      "Epoch 225/300\n",
      "----------\n",
      "train Loss: 9.8826\n",
      "val Loss: 56.6237\n",
      "Time Elapsed: 53m 35s\n",
      "\n",
      "Epoch 226/300\n",
      "----------\n",
      "train Loss: 9.8803\n",
      "val Loss: 56.6251\n",
      "Time Elapsed: 53m 49s\n",
      "\n",
      "Epoch 227/300\n",
      "----------\n",
      "train Loss: 9.8799\n",
      "val Loss: 56.6273\n",
      "Time Elapsed: 54m 3s\n",
      "\n",
      "Epoch 228/300\n",
      "----------\n",
      "train Loss: 9.8794\n",
      "val Loss: 56.6277\n",
      "Time Elapsed: 54m 17s\n",
      "\n",
      "Epoch 229/300\n",
      "----------\n",
      "train Loss: 9.8790\n",
      "val Loss: 56.6281\n",
      "Time Elapsed: 54m 31s\n",
      "\n",
      "Epoch 230/300\n",
      "----------\n",
      "train Loss: 9.8785\n",
      "val Loss: 56.6337\n",
      "Time Elapsed: 54m 45s\n",
      "\n",
      "Epoch 231/300\n",
      "----------\n",
      "train Loss: 9.8781\n",
      "val Loss: 56.6319\n",
      "Time Elapsed: 54m 60s\n",
      "\n",
      "Epoch 232/300\n",
      "----------\n",
      "train Loss: 9.8778\n",
      "val Loss: 56.6308\n",
      "Time Elapsed: 55m 14s\n",
      "\n",
      "Epoch 233/300\n",
      "----------\n",
      "train Loss: 9.8774\n",
      "val Loss: 56.6341\n",
      "Time Elapsed: 55m 28s\n",
      "\n",
      "Epoch 234/300\n",
      "----------\n",
      "train Loss: 9.8769\n",
      "val Loss: 56.6320\n",
      "Time Elapsed: 55m 42s\n",
      "\n",
      "Epoch 235/300\n",
      "----------\n",
      "train Loss: 9.8765\n",
      "val Loss: 56.6348\n",
      "Time Elapsed: 55m 56s\n",
      "\n",
      "Epoch 236/300\n",
      "----------\n",
      "train Loss: 9.8762\n",
      "val Loss: 56.6380\n",
      "Time Elapsed: 56m 10s\n",
      "\n",
      "Epoch 237/300\n",
      "----------\n",
      "train Loss: 9.8749\n",
      "val Loss: 56.6365\n",
      "Time Elapsed: 56m 25s\n",
      "\n",
      "Epoch 238/300\n",
      "----------\n",
      "train Loss: 9.8746\n",
      "val Loss: 56.6363\n",
      "Time Elapsed: 56m 40s\n",
      "\n",
      "Epoch 239/300\n",
      "----------\n",
      "train Loss: 9.8744\n",
      "val Loss: 56.6375\n",
      "Time Elapsed: 56m 54s\n",
      "\n",
      "Epoch 240/300\n",
      "----------\n",
      "train Loss: 9.8743\n",
      "val Loss: 56.6382\n",
      "Time Elapsed: 57m 8s\n",
      "\n",
      "Epoch 241/300\n",
      "----------\n",
      "train Loss: 9.8741\n",
      "val Loss: 56.6390\n",
      "Time Elapsed: 57m 22s\n",
      "\n",
      "Epoch 242/300\n",
      "----------\n",
      "train Loss: 9.8739\n",
      "val Loss: 56.6403\n",
      "Time Elapsed: 57m 36s\n",
      "\n",
      "Epoch 243/300\n",
      "----------\n",
      "train Loss: 9.8736\n",
      "val Loss: 56.6405\n",
      "Time Elapsed: 57m 50s\n",
      "\n",
      "Epoch 244/300\n",
      "----------\n",
      "train Loss: 9.8735\n",
      "val Loss: 56.6407\n",
      "Time Elapsed: 58m 4s\n",
      "\n",
      "Epoch 245/300\n",
      "----------\n",
      "train Loss: 9.8734\n",
      "val Loss: 56.6407\n",
      "Time Elapsed: 58m 18s\n",
      "\n",
      "Epoch 246/300\n",
      "----------\n",
      "train Loss: 9.8731\n",
      "val Loss: 56.6414\n",
      "Time Elapsed: 58m 32s\n",
      "\n",
      "Epoch 247/300\n",
      "----------\n",
      "train Loss: 9.8729\n",
      "val Loss: 56.6422\n",
      "Time Elapsed: 58m 46s\n",
      "\n",
      "Epoch 248/300\n",
      "----------\n",
      "train Loss: 9.8727\n",
      "val Loss: 56.6448\n",
      "Time Elapsed: 59m 0s\n",
      "\n",
      "Epoch 249/300\n",
      "----------\n",
      "train Loss: 9.8725\n",
      "val Loss: 56.6460\n",
      "Time Elapsed: 59m 14s\n",
      "\n",
      "Epoch 250/300\n",
      "----------\n",
      "train Loss: 9.8723\n",
      "val Loss: 56.6457\n",
      "Time Elapsed: 59m 29s\n",
      "\n",
      "Epoch 251/300\n",
      "----------\n",
      "train Loss: 9.8721\n",
      "val Loss: 56.6456\n",
      "Time Elapsed: 59m 43s\n",
      "\n",
      "Epoch 252/300\n",
      "----------\n",
      "train Loss: 9.8718\n",
      "val Loss: 56.6457\n",
      "Time Elapsed: 59m 58s\n",
      "\n",
      "Epoch 253/300\n",
      "----------\n",
      "train Loss: 9.8717\n",
      "val Loss: 56.6464\n",
      "Time Elapsed: 60m 12s\n",
      "\n",
      "Epoch 254/300\n",
      "----------\n",
      "train Loss: 9.8715\n",
      "val Loss: 56.6482\n",
      "Time Elapsed: 60m 26s\n",
      "\n",
      "Epoch 255/300\n",
      "----------\n",
      "train Loss: 9.8713\n",
      "val Loss: 56.6476\n",
      "Time Elapsed: 60m 40s\n",
      "\n",
      "Epoch 256/300\n",
      "----------\n",
      "train Loss: 9.8710\n",
      "val Loss: 56.6458\n",
      "Time Elapsed: 60m 56s\n",
      "\n",
      "Epoch 257/300\n",
      "----------\n",
      "train Loss: 9.8708\n",
      "val Loss: 56.6475\n",
      "Time Elapsed: 61m 11s\n",
      "\n",
      "Epoch 258/300\n",
      "----------\n",
      "train Loss: 9.8707\n",
      "val Loss: 56.6470\n",
      "Time Elapsed: 61m 25s\n",
      "\n",
      "Epoch 259/300\n",
      "----------\n",
      "train Loss: 9.8705\n",
      "val Loss: 56.6477\n",
      "Time Elapsed: 61m 40s\n",
      "\n",
      "Epoch 260/300\n",
      "----------\n",
      "train Loss: 9.8703\n",
      "val Loss: 56.6478\n",
      "Time Elapsed: 61m 55s\n",
      "\n",
      "Epoch 261/300\n",
      "----------\n",
      "train Loss: 9.8701\n",
      "val Loss: 56.6492\n",
      "Time Elapsed: 62m 10s\n",
      "\n",
      "Epoch 262/300\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# optimiser = torch.optim.SGD(c_model.parameters(), lr=0.01, momentum=0.1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimiser, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m c_model \u001b[38;5;241m=\u001b[39m CTCtrain(c_model, dataloaders, optimiser, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, scheduler\u001b[38;5;241m=\u001b[39mscheduler)\n",
      "Cell \u001b[1;32mIn[47], line 50\u001b[0m, in \u001b[0;36mCTCtrain\u001b[1;34m(model, dataloaders, optimiser, num_epochs, scheduler)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     51\u001b[0m         optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alvin\\.conda\\envs\\NLP\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alvin\\.conda\\envs\\NLP\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alvin\\.conda\\envs\\NLP\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_model = CTCEncoder(input_size=6,\n",
    "                     hidden_size=256,\n",
    "                     num_layers=2,\n",
    "                     output_size=27,\n",
    "                     bidirectional=True).to(device)\n",
    "optimiser = torch.optim.AdamW(c_model.parameters(), lr=0.01)\n",
    "# optimiser = torch.optim.SGD(c_model.parameters(), lr=0.01, momentum=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, factor=0.5)\n",
    "c_model = CTCtrain(c_model, dataloaders, optimiser, num_epochs=300, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
