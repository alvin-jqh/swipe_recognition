{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_torch_dataset import SwipeDataset\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"dataset\")\n",
    "\n",
    "data = SwipeDataset(data_dir=dataset_path,\n",
    "                    batch=False)\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_set, val_set, test_set = random_split(data, [0.8, 0.1, 0.1], generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "reduced_set = Subset(data, range(200))\n",
    "train_set, val_set, test_set = random_split(reduced_set, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    :param batch: List of tuples (input, word, word_tensor)\n",
    "                  - input: (T, 6)\n",
    "                  - word: a string of characters\n",
    "                  - word_tensor: encoded word as indicies with 0 as the blank\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort batch by sequence length (descending order)\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "\n",
    "    inputs, words, targets = zip(*batch)\n",
    "    input_lengths = torch.LongTensor([x.shape[0] for x in inputs])  # store the lengths of inputs\n",
    "    input = pack_sequence(inputs)   # pack the inputs\n",
    "\n",
    "    target_lengths = torch.LongTensor([len(x) for x in words])\n",
    "    targets = torch.cat(targets)    # concatenate all the targets\n",
    "\n",
    "    return input, targets, input_lengths, target_lengths, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "dataloaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [x.shape[1] for x in data[:][0][:]]\n",
    "max(input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_word, test_tensor = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Seq2Seq\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t_model = Seq2Seq(hidden_size=32,\n",
    "                  num_layers=2, \n",
    "                  input_size=6, \n",
    "                  output_size=27).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch_indicies, set, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Takes a batch of variable lengths\n",
    "    \"\"\"\n",
    "    for i in batch_indicies:\n",
    "        input, word, word_tensor = set[i]\n",
    "        input = input.to(device)\n",
    "        word_tensor = word_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_model(model, train_set, val_set, optimiser, criterion=nn.CTCLoss(), batch_size=32, num_epochs=10):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # set the mode of the model based on the phase and change the data used\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                batches = list(range(len(train_set)))\n",
    "                data = train_set\n",
    "            else:\n",
    "                model.eval()\n",
    "                batches = list(range(len(val_set)))\n",
    "                data = val_set\n",
    "\n",
    "            # batch like this because the words are of different lenghts and batching isnt used\n",
    "            random.shuffle(batches)\n",
    "            batches = np.array_split(batches, len(batches) // batch_size)\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for batch in batches:   # iterate over each batch of dataset\n",
    "                batch_loss = 0\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                # enable gradients only if in training mode\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    for i in batch:     # for every datapoint in the batch\n",
    "                        input, word, word_tensor = data[i]\n",
    "\n",
    "                        input = input.to(device)\n",
    "                        word_tensor = word_tensor.to(device)\n",
    "                        word_length = len(word)\n",
    "\n",
    "                        output = t_model(input)\n",
    "                        # rearrange the output for CTC loss\n",
    "                        output = output.permute(1, 0, 2)    # (T, N, C)\n",
    "                        # convert to tensors\n",
    "                        input_lengths = torch.LongTensor([input.shape[1]]).to(device)\n",
    "                        target_lengths = torch.LongTensor([word_length]).to(device)\n",
    "\n",
    "                        loss = criterion(output, word_tensor, input_lengths, target_lengths)\n",
    "                        batch_loss += loss\n",
    "                    # find mean batch loss\n",
    "                    avg_batch_loss = batch_loss / len(batch)\n",
    "\n",
    "                    # back prop only if in train\n",
    "                    if phase == \"train\":\n",
    "                        avg_batch_loss.backward()\n",
    "                        # clip the loss so we dont get exploding gradients\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                        optimiser.step()\n",
    "                # calculates the total loss for epoch over all batches\n",
    "                running_loss += avg_batch_loss.item()\n",
    "\n",
    "            # track epoch oss\n",
    "            if phase == \"train\":\n",
    "                epoch_loss = running_loss / len(batches)\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                epoch_loss = running_loss / len(batches)\n",
    "                val_losses.append(epoch_loss)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f\"Time elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")       \n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Seq2Seq\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t_model = Seq2Seq(hidden_size=32,\n",
    "                  num_layers=2, \n",
    "                  input_size=6, \n",
    "                  output_size=27).to(device)\n",
    "\n",
    "optimiser = torch.optim.SGD(t_model.parameters(), lr=0.01)\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "t_model = train_model(t_model, train_set, val_set, optimiser,criterion, num_epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in, test_word, test_ten = test_set[0]\n",
    "test_in = test_in.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = t_model(test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'_': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8,\n",
    "              'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16,\n",
    "              'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24,\n",
    "              'y': 25, 'z': 26}\n",
    "reversed_vocab = {k: u for u, k in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outputs(decoder_output):\n",
    "    indicies = torch.argmax(decoder_output.squeeze(1), dim=-1).tolist()\n",
    "    words = []\n",
    "    for word in indicies:\n",
    "        characters = [reversed_vocab[i] for i in word]\n",
    "        words.append(characters)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outputs(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_sequence\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class CTCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, output_size=27, bidirectional=True, dropout = 0.5, lstm_dropout = 0):\n",
    "        super(CTCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=False,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=lstm_dropout)\n",
    "        self.layer_norm = nn.LayerNorm(2 * hidden_size if bidirectional else hidden_size)\n",
    "        # self.batch_norm = nn.BatchNorm1d(2 * hidden_size if bidirectional else hidden_size)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size if bidirectional else hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        : param x: (batch_size, seq_len, input_size)\n",
    "        : return logits: (seq_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        lstm_outputs, _ = self.lstm(x)  # lstm_out shape: (seq_len, batch_size, hidden_size*2 if bidirectional)\n",
    "\n",
    "        lstm_out = pad_packed_sequence(lstm_outputs, batch_first=False)[0]\n",
    "\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.drop(lstm_out)\n",
    "\n",
    "        # lstm_out = self.batch_norm(lstm_out.permute(0, 2, 1))\n",
    "        # lstm_out = self.drop(lstm_out.permute(0, 2, 1))\n",
    "\n",
    "        logits = self.fc(lstm_out)  # shape: (batch_size, seq_len, output_size)\n",
    "        return F.log_softmax(logits, dim=-1)  # Log-softmax for CTC loss\n",
    "    \n",
    "c_model = CTCEncoder(input_size=6,\n",
    "                     hidden_size=32,\n",
    "                     num_layers=2,\n",
    "                     output_size=27,\n",
    "                     bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_inputs, t_targets, t_input_lengths, t_target_lengths, t_words = next(iter(train_loader))\n",
    "t_output = c_model(t_inputs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0.05):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.best_validation_loss - self.min_delta:\n",
    "            self.best_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            return False  # No early stopping\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True  # Stop training\n",
    "            return False\n",
    "\n",
    "def CTCtrain(model, dataloaders, set_lengths, optimiser, num_epochs = 5, scheduler = None, patience = 15, min_delta = 0.05):\n",
    "    criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta = min_delta)\n",
    "    stop = False\n",
    "\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        lowest_loss = float(\"Inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, targets, input_lengths, target_lengths, words in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    input_lengths = input_lengths.to(device)\n",
    "                    target_lengths = target_lengths.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimiser.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "                            loss.backward()\n",
    "                            optimiser.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * len(target_lengths)  # multiply by batch size\n",
    "                if phase == 'train' and scheduler is not None:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "                epoch_loss = running_loss / set_lengths[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    train_loss.append(epoch_loss)\n",
    "                else:\n",
    "                    val_loss.append(epoch_loss)\n",
    "                \n",
    "                # early stop based on validation loss\n",
    "                if phase == 'val':\n",
    "                    stop = early_stopper.early_stop(epoch_loss)\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_loss < lowest_loss:\n",
    "                    lowest_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                \n",
    "            if stop:\n",
    "                break\n",
    "            \n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Time Elapsed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Lowest Loss: {lowest_loss:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "\n",
    "        plt.plot(train_loss, label=\"Training Loss\")\n",
    "        plt.plot(val_loss, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = CTCEncoder(input_size=6,\n",
    "                     hidden_size= 256,\n",
    "                     num_layers=4,\n",
    "                     output_size=27,\n",
    "                     bidirectional=True,\n",
    "                     dropout=0.3,\n",
    "                     lstm_dropout=0.3).to(device)\n",
    "\n",
    "set_lengths = {\"train\":len(train_set),\n",
    "               \"val\":len(val_set)}\n",
    "optimiser = torch.optim.AdamW(c_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# optimiser = torch.optim.SGD(c_model.parameters(), lr=0.01, momentum=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, patience=5, factor=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser,step_size=10, gamma=0.1)\n",
    "c_model = CTCtrain(c_model, dataloaders,set_lengths, optimiser, num_epochs=150, scheduler=scheduler, patience = 5, min_delta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_file = \"lstm_n.pt\"\n",
    "save_path = os.path.join(os.getcwd(), \"models\", model_file)\n",
    "torch.save(c_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_file = \"lstm_n.pt\"\n",
    "save_path = os.path.join(os.getcwd(), \"models\", model_file)\n",
    "\n",
    "c_model = torch.load(save_path, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, datapoints):\n",
    "    criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    model_outputs = []\n",
    "    ground_truth = []\n",
    "    # set the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, targets, input_lengths, target_lengths, words in test_data:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        model_outputs.append(outputs.cpu())\n",
    "        ground_truth.append(words)\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        \n",
    "        running_loss += loss * len(target_lengths)  # multiply by batch size\n",
    "\n",
    "    avg_loss = running_loss / datapoints    # average over the entire test set\n",
    "\n",
    "    print(f\"Average test loss: {avg_loss}\")\n",
    "\n",
    "    return model_outputs, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, truth = test_model(c_model, test_loader, len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'_': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8,\n",
    "              'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16,\n",
    "              'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24,\n",
    "              'y': 25, 'z': 26}\n",
    "reversed_vocab = {k: u for u, k in vocabulary.items()}\n",
    "\n",
    "def greedy_decode(logits, ground_truth):\n",
    "    decoded_words = []\n",
    "    for batch, words in zip(logits, ground_truth):\n",
    "        batched_results = []\n",
    "\n",
    "        # for each sequence in the batch\n",
    "        for b in range(batch.shape[1]):\n",
    "            decoded_word = \"\"\n",
    "            # at each timestep or letter\n",
    "            for t in range(batch.shape[0]):\n",
    "                timestep_logits = batch[t, b, :]   # get all the logits in that time step\n",
    "\n",
    "                # Find the index of the maximum logit\n",
    "                predicted_index = torch.argmax(timestep_logits).item()\n",
    "\n",
    "                # Convert the index to a character using the reversed vocabulary\n",
    "                predicted_char = reversed_vocab.get(predicted_index, '_') # Default to '_' if index not found\n",
    "\n",
    "                if predicted_char != \"_\":\n",
    "                    # Append the character to the decoded word\n",
    "                    decoded_word += predicted_char\n",
    "\n",
    "            batched_results.append(decoded_word)\n",
    "        decoded_words.append(batched_results)\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "\n",
    "def beam_search(logits, ground_truth, beam_width=3):\n",
    "    \"\"\"\n",
    "    Decodes a tensor of logits using beam search, adapted for the provided input structure.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): A tensor of logits with shape (sequence_length, batch_size, vocab_size).\n",
    "        ground_truth (list): A list of ground truth words (not directly used in beam search but included for consistency).\n",
    "        reversed_vocab (dict): A dictionary mapping vocabulary indices to characters.\n",
    "        beam_width (int): The width of the beam.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of decoded strings, one for each sequence in the batch.\n",
    "    \"\"\"\n",
    "    decoded_words = []\n",
    "    for batch, _ in zip(logits, ground_truth):  # ground_truth is not used in the beam search.\n",
    "        batched_results = []\n",
    "\n",
    "        for b in range(batch.shape[1]):\n",
    "            initial_beam = [([(0.0, \"\")], 0.0)]\n",
    "            final_beams = []\n",
    "\n",
    "            for t in range(batch.shape[0]):\n",
    "                new_beam = []\n",
    "                for seq, total_log_prob in initial_beam:\n",
    "                    timestep_logits = batch[t, b, :]\n",
    "\n",
    "                    topk_probs, topk_indices = torch.topk(timestep_logits, beam_width)\n",
    "\n",
    "                    for i in range(beam_width):\n",
    "                        char_index = topk_indices[i].item()\n",
    "                        char = reversed_vocab.get(char_index, '_')\n",
    "                        new_seq = seq + [(topk_probs[i].item(), char)]\n",
    "                        new_total_log_prob = total_log_prob + topk_probs[i].item()\n",
    "                        new_beam.append((new_seq, new_total_log_prob))\n",
    "\n",
    "                initial_beam = heapq.nlargest(beam_width, new_beam, key=lambda x: x[1])\n",
    "\n",
    "            best_sequence, _ = max(initial_beam, key=lambda x: x[1])\n",
    "            decoded_word = \"\".join([char for log_prob, char in best_sequence[1:] if char != \"_\"])\n",
    "            batched_results.append(decoded_word)\n",
    "\n",
    "        decoded_words.append(batched_results)\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_words = greedy_decode(logits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_words = beam_search(logits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, truth):\n",
    "    exact_match = 0\n",
    "    length_match = 0\n",
    "    same_first = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            pred = predictions[i][j]\n",
    "            ground = truth[i][j]\n",
    "            total += 1\n",
    "\n",
    "            if pred == ground:\n",
    "                exact_match += 1\n",
    "            \n",
    "            if len(pred) == len(ground):\n",
    "                length_match += 1\n",
    "            \n",
    "            if pred[0] == ground[0]:\n",
    "                same_first += 1\n",
    "    \n",
    "\n",
    "    EM = exact_match / total\n",
    "    LM = length_match / total\n",
    "    FM = same_first / total\n",
    "\n",
    "    print(f\"Exact Match: {EM}\")\n",
    "    print(f\"Correct Length: {LM}\")\n",
    "    print(f\"Correct First Letter: {FM}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(beam_words, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
